{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Yelp Dataset: Data Loading and Neo4j Import\n",
                "\n",
                "This notebook processes Yelp dataset JSON files (Business, User, Review, Tip), cleans the data, transforms it into CSV format suitable for Neo4j's `neo4j-admin import` tool, and imports it into a Neo4j database."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Setup: Imports and Constants"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Core Libraries\n",
                "import json\n",
                "import csv\n",
                "import uuid\n",
                "import os\n",
                "import regex as re\n",
                "import subprocess\n",
                "import datetime\n",
                "import shutil\n",
                "from pathlib import Path\n",
                "from time import sleep\n",
                "from typing import List, Tuple, Set, Dict\n",
                "\n",
                "# Data Handling & Graph\n",
                "import numpy as np\n",
                "import pandas as pd\n",
                "import reverse_geocoder as rg\n",
                "from py2neo import Graph\n",
                "# Import base Neo4jError and specific retryable errors\n",
                "from neo4j.exceptions import ServiceUnavailable, TransientError, Neo4jError\n",
                "from textblob import TextBlob # Added for sentiment analysis\n",
                "from collections import defaultdict # Added for efficient aggregation"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [],
            "source": [
                "# --- Constants: Node Labels ---\n",
                "BUSINESS_NODE = \"Business\"\n",
                "USER_NODE = \"User\"\n",
                "REVIEW_NODE = \"Review\"\n",
                "TIP_NODE = \"Tip\" # Added for tips\n",
                "CATEGORY_NODE = \"Category\"\n",
                "CITY_NODE = \"City\"\n",
                "STATE_NODE = \"State\"  # Changed from Area\n",
                "COUNTRY_NODE = \"Country\"\n",
                "\n",
                "# --- Constants: Relationship Types ---\n",
                "IN_CATEGORY = \"IN_CATEGORY\"\n",
                "IN_CITY = \"IN_CITY\"\n",
                "IN_STATE = \"IN_STATE\"   \n",
                "IN_COUNTRY = \"IN_COUNTRY\"\n",
                "FRIENDS = \"FRIENDS\"\n",
                "REVIEWS = \"REVIEWS\" # Business <-[:REVIEWS]- Review/Tip\n",
                "WROTE = \"WROTE\"     # User -[:WROTE]-> Review/Tip"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Configuration: File Paths and Neo4j Connection"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# --- Input Data Files ---\n",
                "# Adjusted path to point one level up and into 'yelp_dataset'\n",
                "data_folder = \"../cleaned_yelp_data\" \n",
                "business_json_file = os.path.join(data_folder, \"cleaned_yelp_academic_dataset_business.json\")\n",
                "review_json_file = os.path.join(data_folder, \"cleaned_yelp_academic_dataset_review.json\")\n",
                "user_json_file = os.path.join(data_folder, \"cleaned_yelp_academic_dataset_user.json\")\n",
                "tip_json_file = os.path.join(data_folder, \"cleaned_yelp_academic_dataset_tip.json\") # Added tip file\n",
                "\n",
                "# List of raw input files\n",
                "list_raw_files = [business_json_file, review_json_file, user_json_file, tip_json_file] # Added tip file\n",
                "\n",
                "# --- Intermediate Fixed Data Files ---\n",
                "# These will be created in the same directory as the raw files\n",
                "fixed_business_json_file = os.path.join(data_folder, \"fixed_yelp_academic_dataset_business.json\")\n",
                "fixed_review_json_file = os.path.join(data_folder, \"fixed_yelp_academic_dataset_review.json\")\n",
                "fixed_user_json_file = os.path.join(data_folder, \"fixed_yelp_academic_dataset_user.json\")\n",
                "fixed_tip_json_file = os.path.join(data_folder, \"fixed_yelp_academic_dataset_tip.json\") # Added fixed tip file\n",
                "\n",
                "# List of fixed intermediate files\n",
                "list_fixed_data = [fixed_business_json_file, fixed_review_json_file, fixed_user_json_file, fixed_tip_json_file] # Added fixed tip file\n",
                "\n",
                "# --- Output CSV Files for Neo4j Import ---\n",
                "# These will be generated in the current working directory (assumed project root)\n",
                "business_nodes_csv_file = \"business_nodes.csv\"\n",
                "category_nodes_csv_file = \"category_nodes.csv\"\n",
                "city_nodes_csv_file = \"city_nodes.csv\"\n",
                "state_nodes_csv_file = \"state_nodes.csv\"  # Changed from area_nodes.csv\n",
                "country_nodes_csv_file = \"country_nodes.csv\"\n",
                "user_nodes_csv_file = \"user_nodes.csv\"\n",
                "review_nodes_csv_file = \"review_nodes.csv\"\n",
                "tip_nodes_csv_file = \"tip_nodes.csv\" # Added tip nodes file\n",
                "relationship_csv_file = \"relationships.csv\"\n",
                "\n",
                "# List of node CSV files (updated)\n",
                "nodes_files = [\n",
                "    business_nodes_csv_file, category_nodes_csv_file, city_nodes_csv_file, \n",
                "    state_nodes_csv_file, country_nodes_csv_file, user_nodes_csv_file, \n",
                "    review_nodes_csv_file, tip_nodes_csv_file # Added tip nodes file\n",
                "]\n",
                "\n",
                "# --- Neo4j Configuration ---\n",
                "graph_name = \"neo4j\" # Default graph name\n",
                "SERVER_ADDRESS = \"bolt://localhost:7687\"\n",
                "SERVER_AUTH = (\"neo4j\",\"password\") # Replace with your Neo4j credentials\n",
                "\n",
                "# --- Neo4j Installation Path (Manual Configuration Required for Neo4j 5+) ---\n",
                "# !! IMPORTANT !! Set this variable to the root directory of your Neo4j installation.\n",
                "# Example for Windows: neo4j_home = r\"C:\\Users\\YourUser\\.Neo4jDesktop\\relate-data\\dbmss\\dbms-abc12345-def67890\"\n",
                "# Example for Linux/macOS: neo4j_home = \"/path/to/neo4j-community-5.x.x\"\n",
                "neo4j_home = \"C:/Users/wiztu/.Neo4jDesktop/relate-data/dbmss/dbms-d9cecb50-c260-4dd0-b3e0-6b387c58d5ec\" \n",
                "\n",
                "if neo4j_home is None:\n",
                "    print(\"WARNING: 'neo4j_home' variable is not set.\")\n",
                "    print(\"         Please edit this cell and set it to your Neo4j installation path.\")\n",
                "    print(\"         The script needs this path to find 'neo4j-admin' and 'neo4j' commands.\")\n",
                "\n",
                "# --- Utility Function ---\n",
                "def delete_files(files: List[str]):\n",
                "    \"\"\"Safely deletes a list of files if they exist.\"\"\"\n",
                "    for one_file in files:\n",
                "        one_path = Path(one_file)\n",
                "        if one_path.is_file():\n",
                "            try:\n",
                "                one_path.unlink()\n",
                "                # print(f\"Deleted file: {one_file}\")\n",
                "            except OSError as e:\n",
                "                print(f\"Error deleting file {one_file}: {e}\")\n",
                "\n",
                "# --- Initial Check: Raw Data Files ---\n",
                "if not all(Path(f).is_file() for f in list_raw_files):\n",
                "    missing = [f for f in list_raw_files if not Path(f).is_file()]\n",
                "    raise FileNotFoundError(f\"Missing raw Yelp JSON files: {missing}. Expected in '{data_folder}'.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Preprocessing Raw Data\n",
                "\n",
                "**Data Challenges Addressed:**\n",
                "1.  **Non-Unique IDs:** `business_id`, `user_id`, `review_id`, and potentially tip identifiers (if they existed) can overlap across files. Prefixes (`b-`, `u-`, `r-`, `t-`) are added to ensure global uniqueness for Neo4j import. Tips require UUID generation as they lack a source ID.\n",
                "2.  **Dangling Friendships:** Users might list friends who don't exist in the dataset. These non-existent friend references are removed.\n",
                "3.  **Duplicate Categories:** Businesses might list the same category multiple times. Duplicates are removed.\n",
                "4.  **String vs. Array:** `friends` (in User) and `categories` (in Business) are stored as comma-separated strings instead of proper JSON arrays. While fixing IDs/categories, these are handled appropriately."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {},
            "outputs": [],
            "source": [
                "# --- Preprocessing Functions ---\n",
                "\n",
                "def remove_unknown_friends(raw_user_path: str, output_path: str) -> None:\n",
                "    \"\"\"Reads raw user data, identifies all valid user IDs, \n",
                "       and writes a new file containing only users with valid friend lists.\"\"\"\n",
                "    print(f\"Reading user IDs from {raw_user_path}...\")\n",
                "    user_ids = set()\n",
                "    try:\n",
                "        with open(raw_user_path, \"r\", encoding=\"utf-8\") as rf:\n",
                "            for i, line in enumerate(rf):\n",
                "                if len(line.strip()) > 0:\n",
                "                    try:\n",
                "                        json_node = json.loads(line)\n",
                "                        user_ids.add(json_node[\"user_id\"])\n",
                "                    except json.JSONDecodeError:\n",
                "                        print(f\"Warning: Skipping invalid JSON on line {i+1} in {raw_user_path}\")\n",
                "    except FileNotFoundError:\n",
                "        print(f\"Error: File not found {raw_user_path}\")\n",
                "        raise\n",
                "    print(f\"Found {len(user_ids)} unique user IDs.\")\n",
                "\n",
                "    print(f\"Writing users with cleaned friend lists to {output_path}...\")\n",
                "    try:\n",
                "        with open(raw_user_path, \"r\", encoding=\"utf-8\") as rf, open(output_path, mode=\"w\", encoding=\"utf-8\") as of:\n",
                "            for i, line in enumerate(rf):\n",
                "                if len(line.strip()) > 0:\n",
                "                    try:\n",
                "                        json_node = json.loads(line)\n",
                "                        friends_str = json_node.get(\"friends\", \"\")\n",
                "                        if friends_str and friends_str.lower() != 'none' and len(friends_str.strip()) > 0:\n",
                "                            friends_arr = re.split(r\"\\s*,\\s*\", friends_str.strip())\n",
                "                            # Filter out friends not present in the dataset\n",
                "                            friends_exist_arr = [f for f in friends_arr if f in user_ids]\n",
                "                            json_node[\"friends\"] = ', '.join(friends_exist_arr)\n",
                "                        else:\n",
                "                            json_node[\"friends\"] = \"\" # Ensure field exists but is empty\n",
                "                        of.write(f'{json.dumps(json_node)}\\n')\n",
                "                    except json.JSONDecodeError:\n",
                "                        # Warning already printed during ID collection\n",
                "                        pass \n",
                "    except FileNotFoundError:\n",
                "        print(f\"Error: File not found {raw_user_path} during writing phase.\")\n",
                "        raise\n",
                "    print(\"Finished cleaning friend lists.\")\n",
                "\n",
                "def make_ids_unique(input_path: str, output_path: str, func_to_modify) -> None:\n",
                "    \"\"\"Applies a modification function (e.g., adding ID prefixes) \n",
                "       to each JSON object in the input file and writes to the output file.\"\"\"\n",
                "    print(f\"Applying ID modifications to {input_path} -> {output_path}...\")\n",
                "    temp_output_file = output_path + \".tmp-\" + str(uuid.uuid4())\n",
                "    try:\n",
                "        with open(input_path, mode=\"r\", encoding=\"utf-8\") as in_f, open(temp_output_file, mode=\"w\", encoding=\"utf-8\") as ou_f:\n",
                "            for i, line in enumerate(in_f):\n",
                "                if len(line.strip()) > 0:\n",
                "                    try:\n",
                "                        json_node = json.loads(line)\n",
                "                        json_node = func_to_modify(json_node)\n",
                "                        ou_f.write(f'{json.dumps(json_node)}\\n')\n",
                "                    except json.JSONDecodeError:\n",
                "                         print(f\"Warning: Skipping invalid JSON on line {i+1} in {input_path}\")\n",
                "    except FileNotFoundError:\n",
                "        print(f\"Error: File not found {input_path}\")\n",
                "        # Clean up temp file if it exists\n",
                "        if Path(temp_output_file).is_file():\n",
                "            Path(temp_output_file).unlink()\n",
                "        raise\n",
                "        \n",
                "    # Replace original file with modified temp file\n",
                "    try:\n",
                "        os.replace(temp_output_file, output_path)\n",
                "        print(f\"Successfully updated {output_path}.\")\n",
                "    except OSError as e:\n",
                "        print(f\"Error replacing file {output_path} with {temp_output_file}: {e}\")\n",
                "        # Attempt cleanup again\n",
                "        if Path(temp_output_file).is_file():\n",
                "            try: Path(temp_output_file).unlink()\n",
                "            except OSError: pass\n",
                "        raise"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Fixed data files already exist. Skipping preprocessing.\n"
                    ]
                }
            ],
            "source": [
                "# --- Preprocessing Execution ---\n",
                "\n",
                "# Check if fixed files already exist. If not, create them.\n",
                "if not all(Path(f).is_file() for f in list_fixed_data):\n",
                "    print(\"Fixed data files not found. Starting preprocessing...\")\n",
                "    # Remove any potentially incomplete fixed files first\n",
                "    delete_files(list_fixed_data)\n",
                "\n",
                "    # --- Define Modification Functions ---\n",
                "    def fix_user(json_node):\n",
                "        # Prefix user_id\n",
                "        original_user_id = json_node[\"user_id\"]\n",
                "        json_node[\"user_id\"] = \"u-\" + original_user_id\n",
                "        # Prefix friend_ids (already cleaned in remove_unknown_friends)\n",
                "        friends_str = json_node.get(\"friends\", \"\")\n",
                "        if friends_str and len(friends_str.strip()) > 0:\n",
                "            friends_arr = re.split(r\"\\s*,\\s*\", friends_str.strip())\n",
                "            # Prefix each valid friend ID\n",
                "            friends_arr = [\"u-\" + f_id for f_id in friends_arr]\n",
                "            json_node[\"friends\"] = ', '.join(friends_arr)\n",
                "        else:\n",
                "             json_node[\"friends\"] = \"\"\n",
                "        return json_node\n",
                "\n",
                "    def fix_business(json_node):\n",
                "        # Prefix business_id\n",
                "        json_node[\"business_id\"] = \"b-\" + json_node[\"business_id\"]\n",
                "        # Deduplicate and clean categories\n",
                "        categories_str = json_node.get(\"categories\", None)\n",
                "        if categories_str and len(categories_str.strip()) > 0:\n",
                "            categories_arr = re.split(r\"\\s*,\\s*\", categories_str.strip())\n",
                "            # Filter out empty strings and trim whitespace, then deduplicate\n",
                "            categories_set = set(cat.strip() for cat in categories_arr if cat and len(cat.strip()) > 0)\n",
                "            json_node[\"categories\"] = ', '.join(sorted(list(categories_set))) # Sort for consistency\n",
                "        else:\n",
                "            json_node[\"categories\"] = \"\"\n",
                "        return json_node\n",
                "\n",
                "    def fix_review(json_node):\n",
                "        # Prefix review_id, user_id, business_id\n",
                "        json_node[\"review_id\"] = \"r-\" + json_node[\"review_id\"]\n",
                "        json_node[\"user_id\"] = \"u-\" + json_node[\"user_id\"]\n",
                "        json_node[\"business_id\"] = \"b-\" + json_node[\"business_id\"]\n",
                "        return json_node\n",
                "    \n",
                "    def fix_tip(json_node):\n",
                "        # Prefix user_id, business_id and generate tip_id\n",
                "        json_node[\"tip_id\"] = \"t-\" + str(uuid.uuid4()) # Generate unique ID for tip\n",
                "        json_node[\"user_id\"] = \"u-\" + json_node[\"user_id\"]\n",
                "        json_node[\"business_id\"] = \"b-\" + json_node[\"business_id\"]\n",
                "        return json_node\n",
                "\n",
                "    # --- Apply Fixes ---\n",
                "    # 1. Clean user friends (reads raw, writes to fixed path)\n",
                "    remove_unknown_friends(user_json_file, fixed_user_json_file)\n",
                "    # 2. Make user IDs unique (reads fixed, modifies in-place)\n",
                "    make_ids_unique(fixed_user_json_file, fixed_user_json_file, fix_user)\n",
                "    # 3. Make business IDs unique and clean categories (reads raw, writes to fixed path)\n",
                "    make_ids_unique(business_json_file, fixed_business_json_file, fix_business)\n",
                "    # 4. Make review IDs unique (reads raw, writes to fixed path)\n",
                "    make_ids_unique(review_json_file, fixed_review_json_file, fix_review)\n",
                "    # 5. Make tip IDs unique (reads raw, writes to fixed path)\n",
                "    make_ids_unique(tip_json_file, fixed_tip_json_file, fix_tip) # Added tip processing\n",
                "    \n",
                "    print(\"Preprocessing complete. Fixed data files created.\")\n",
                "else:\n",
                "    print(\"Fixed data files already exist. Skipping preprocessing.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Generating CSV Files for Neo4j Import Tool\n",
                "\n",
                "This section reads the preprocessed (`fixed_...json`) files and generates CSV files formatted according to the requirements of `neo4j-admin import`.\n",
                "- Node files contain headers like `nodeId:ID`, `propertyName`, `:LABEL`.\n",
                "- The relationship file contains headers `:START_ID`, `:END_ID`, `:TYPE`."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "metadata": {},
            "outputs": [],
            "source": [
                "# --- CSV Generation Functions ---\n",
                "\n",
                "# Global sets to store unique nodes and relationships across functions\n",
                "business_lat_lon = {}\n",
                "state_nodes: Set[Tuple[str, str]] = set() # Changed from area_nodes\n",
                "city_nodes: Set[Tuple[str, str]] = set()\n",
                "country_nodes: Set[str] = set()\n",
                "categories_nodes: Set[str] = set()\n",
                "valid_user_ids: Set[str] = set() # Added to track valid users for tips\n",
                "valid_business_ids: Set[str] = set() # Added to track valid businesses for tips\n",
                "\n",
                "# Relationship sets (tuples: start_id, end_id, type)\n",
                "in_city_relationships: Set[Tuple[str, str, str]] = set()\n",
                "in_state_relationships: Set[Tuple[str, str, str]] = set() # Changed from in_area\n",
                "in_country_relationships: Set[Tuple[str, str, str]] = set()\n",
                "in_category_relationships: Set[Tuple[str, str, str]] = set()\n",
                "friend_relationships: Set[Tuple[str, str, str]] = set()\n",
                "wrote_relationships: Set[Tuple[str, str, str]] = set() # User->Review\n",
                "reviews_relationships: Set[Tuple[str, str, str]] = set() # Review->Business\n",
                "tip_wrote_relationships: Set[Tuple[str, str, str]] = set() # User->Tip (Added)\n",
                "tip_reviews_relationships: Set[Tuple[str, str, str]] = set() # Tip->Business (Added)\n",
                "\n",
                "def process_business_data(business_star_aggregates: Dict):\n",
                "    \"\"\"Reads fixed business data, populates business nodes (with avgStar), categories, \n",
                "       lat/lon mapping, IN_CATEGORY relationships, and valid_business_ids set.\"\"\"\n",
                "    print(f\"Processing business data from {fixed_business_json_file}...\")\n",
                "    global categories_nodes, business_lat_lon, in_category_relationships, valid_business_ids\n",
                "    processed_count = 0\n",
                "    valid_business_ids.clear() # Ensure set is empty before processing\n",
                "    try:\n",
                "        with open(fixed_business_json_file, \"r\", encoding=\"utf-8\") as bjf:\n",
                "            # Prepare writer for business nodes CSV\n",
                "            with open(business_nodes_csv_file, mode=\"w\", encoding=\"utf-8\", newline=\"\") as business_csv:\n",
                "                fieldnames = [\"business_id:ID\", \"name\", \"address\", \"review_count:long\", \"avgStar:float\", \":LABEL\"]\n",
                "                writer = csv.DictWriter(business_csv, fieldnames=fieldnames, delimiter=\",\", quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
                "                writer.writeheader()\n",
                "                \n",
                "                for line in bjf:\n",
                "                    line = line.strip()\n",
                "                    if len(line) > 0:\n",
                "                        try:\n",
                "                            json_node = json.loads(line)\n",
                "                            business_id = json_node[\"business_id\"] # Already prefixed\n",
                "                            \n",
                "                            # Calculate average stars\n",
                "                            agg = business_star_aggregates.get(business_id, {'total_stars': 0, 'review_count': 0})\n",
                "                            avg_stars = agg['total_stars'] / agg['review_count'] if agg['review_count'] > 0 else 0.0\n",
                "                            \n",
                "                            # Write business node\n",
                "                            writer.writerow({\n",
                "                                \"business_id:ID\": business_id,\n",
                "                                \"name\": json_node.get(\"name\", \"\"),\n",
                "                                \"address\": json_node.get(\"address\", \"\"),\n",
                "                                \"review_count:long\": json_node.get(\"review_count\", 0),\n",
                "                                \"avgStar:float\": avg_stars,\n",
                "                                \":LABEL\": BUSINESS_NODE\n",
                "                            })\n",
                "                            \n",
                "                            # Add to valid IDs set\n",
                "                            valid_business_ids.add(business_id)\n",
                "                            \n",
                "                            # Store lat/lon for location processing\n",
                "                            if json_node.get(\"latitude\") is not None and json_node.get(\"longitude\") is not None:\n",
                "                                business_lat_lon[business_id] = (json_node[\"latitude\"], json_node[\"longitude\"])\n",
                "                            \n",
                "                            # Process categories\n",
                "                            categories_str = json_node.get(\"categories\", \"\")\n",
                "                            if categories_str and len(categories_str.strip()) > 0:\n",
                "                                cur_categories = re.split(r\"\\s*,\\s*\", categories_str.strip()) # Already cleaned/deduplicated\n",
                "                                categories_nodes.update(cur_categories)\n",
                "                                for category in cur_categories:\n",
                "                                    in_category_relationships.add((business_id, category, IN_CATEGORY))\n",
                "                            processed_count += 1\n",
                "                        except json.JSONDecodeError:\n",
                "                            print(f\"Warning: Skipping invalid JSON line in {fixed_business_json_file}\")\n",
                "                        except KeyError as e:\n",
                "                            print(f\"Warning: Missing key {e} in business record: {line[:100]}...\")\n",
                "                            \n",
                "    except FileNotFoundError:\n",
                "        print(f\"Error: File not found {fixed_business_json_file}\")\n",
                "        raise\n",
                "    print(f\"Finished processing {processed_count} businesses. Found {len(valid_business_ids)} valid business IDs.\")\n",
                "\n",
                "def process_location_data():\n",
                "    \"\"\"Uses reverse_geocoder on business lat/lons to create City, State, Country nodes \n",
                "       and IN_CITY, IN_STATE, IN_COUNTRY relationships.\"\"\"\n",
                "    print(\"Processing location data using reverse geocoding...\")\n",
                "    global city_nodes, state_nodes, country_nodes, in_city_relationships, in_state_relationships, in_country_relationships\n",
                "    \n",
                "    if not business_lat_lon:\n",
                "        print(\"Warning: No business latitude/longitude data found. Skipping location processing.\")\n",
                "        return\n",
                "        \n",
                "    # Prepare coordinates for batch reverse geocoding\n",
                "    business_ids = list(business_lat_lon.keys())\n",
                "    coordinates = list(business_lat_lon.values())\n",
                "    \n",
                "    print(f\"Performing reverse geocoding for {len(coordinates)} coordinates...\")\n",
                "    try:\n",
                "        location_results = rg.search(coordinates)\n",
                "    except Exception as e:\n",
                "        print(f\"Error during reverse geocoding: {e}\")\n",
                "        print(\"Ensure 'reverse_geocoder' library and its data are installed correctly.\")\n",
                "        raise\n",
                "        \n",
                "    print(\"Processing geocoding results...\")\n",
                "    # Process results and build node/relationship sets\n",
                "    for business_id, loc_info in zip(business_ids, location_results):\n",
                "        city = loc_info.get('name', 'UnknownCity')\n",
                "        state = loc_info.get('admin1', 'UnknownState') # admin1 is typically state/province\n",
                "        country = loc_info.get('cc', 'UnknownCountry') # cc is country code\n",
                "        \n",
                "        # Create unique IDs\n",
                "        # Ensure IDs don't clash if names are identical across different levels\n",
                "        country_id = country\n",
                "        state_id = f\"{state}-{country_id}\"\n",
                "        city_id = f\"{city}-{state_id}\"\n",
                "        \n",
                "        # Add nodes (sets handle uniqueness)\n",
                "        country_nodes.add(country_id)\n",
                "        state_nodes.add((state_id, state))\n",
                "        city_nodes.add((city_id, city))\n",
                "        \n",
                "        # Add relationships (sets handle uniqueness)\n",
                "        in_city_relationships.add((business_id, city_id, IN_CITY))\n",
                "        in_state_relationships.add((city_id, state_id, IN_STATE))\n",
                "        in_country_relationships.add((state_id, country_id, IN_COUNTRY))\n",
                "        \n",
                "    print(f\"Finished processing locations: {len(city_nodes)} cities, {len(state_nodes)} states, {len(country_nodes)} countries.\")\n",
                "\n",
                "def process_user_data():\n",
                "    \"\"\"Reads fixed user data, populates user nodes, FRIEND relationships, and valid_user_ids set.\"\"\"\n",
                "    print(f\"Processing user data from {fixed_user_json_file}...\")\n",
                "    global friend_relationships, valid_user_ids\n",
                "    processed_count = 0\n",
                "    valid_user_ids.clear() # Ensure set is empty before processing\n",
                "    try:\n",
                "        with open(fixed_user_json_file, \"r\", encoding=\"utf-8\") as ujf:\n",
                "            # Prepare writer for user nodes CSV\n",
                "            with open(user_nodes_csv_file, mode=\"w\", encoding=\"utf-8\", newline=\"\") as user_csv:\n",
                "                fieldnames = [\"user_id:ID\", \"name\", \"yelping_since\", \":LABEL\"]\n",
                "                writer = csv.DictWriter(user_csv, fieldnames=fieldnames, delimiter=\",\", quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
                "                writer.writeheader()\n",
                "                \n",
                "                for line in ujf:\n",
                "                    line = line.strip()\n",
                "                    if len(line) > 0:\n",
                "                        try:\n",
                "                            json_node = json.loads(line)\n",
                "                            user_id = json_node[\"user_id\"] # Already prefixed\n",
                "                            \n",
                "                            # Write user node\n",
                "                            writer.writerow({\n",
                "                                \"user_id:ID\": user_id,\n",
                "                                \"name\": json_node.get(\"name\", \"\"),\n",
                "                                \"yelping_since\": json_node.get(\"yelping_since\", \"\"),\n",
                "                                \":LABEL\": USER_NODE\n",
                "                            })\n",
                "                            \n",
                "                            # Add to valid IDs set\n",
                "                            valid_user_ids.add(user_id)\n",
                "                            \n",
                "                            # Process friends\n",
                "                            friends_str = json_node.get(\"friends\", \"\")\n",
                "                            if friends_str and len(friends_str.strip()) > 0:\n",
                "                                friends_arr = re.split(r\"\\s*,\\s*\", friends_str.strip()) # Already cleaned and prefixed\n",
                "                                for friend_id in friends_arr:\n",
                "                                    # Ensure relationship uniqueness (u1 < u2)\n",
                "                                    u1 = min(user_id, friend_id)\n",
                "                                    u2 = max(user_id, friend_id)\n",
                "                                    if u1 != u2: # Avoid self-loops if data error exists\n",
                "                                        friend_relationships.add((u1, u2, FRIENDS))\n",
                "                            processed_count += 1\n",
                "                        except json.JSONDecodeError:\n",
                "                            print(f\"Warning: Skipping invalid JSON line in {fixed_user_json_file}\")\n",
                "                        except KeyError as e:\n",
                "                            print(f\"Warning: Missing key {e} in user record: {line[:100]}...\")\n",
                "                            \n",
                "    except FileNotFoundError:\n",
                "        print(f\"Error: File not found {fixed_user_json_file}\")\n",
                "        raise\n",
                "    print(f\"Finished processing {processed_count} users. Found {len(valid_user_ids)} valid user IDs.\")\n",
                "\n",
                "def process_review_data_with_polarity_and_aggregation() -> Dict:\n",
                "    \"\"\"Reads fixed review data, calculates polarity, aggregates stars by business,\n",
                "       populates review nodes (with polarity), WROTE and REVIEWS relationships,\n",
                "       and returns the star aggregates.\"\"\"\n",
                "    print(f\"Processing review data from {fixed_review_json_file}...\")\n",
                "    global wrote_relationships, reviews_relationships\n",
                "    processed_count = 0\n",
                "    business_star_aggregates = defaultdict(lambda: {'total_stars': 0, 'review_count': 0})\n",
                "    try:\n",
                "        with open(fixed_review_json_file, \"r\", encoding=\"utf-8\") as rjf:\n",
                "            with open(review_nodes_csv_file, mode=\"w\", encoding=\"utf-8\", newline=\"\") as review_csv:\n",
                "                fieldnames = [\"review_id:ID\", \"stars\", \"date\", \"text\", \"useful:long\", \"funny:long\", \"cool:long\", \"polarity:float\", \":LABEL\"]\n",
                "                writer = csv.DictWriter(review_csv, fieldnames=fieldnames, delimiter=\",\", quotechar='\"', quoting=csv.QUOTE_ALL)\n",
                "                writer.writeheader()\n",
                "                \n",
                "                for line in rjf:\n",
                "                    line = line.strip()\n",
                "                    if len(line) > 0:\n",
                "                        try:\n",
                "                            json_node = json.loads(line)\n",
                "                            review_id = json_node[\"review_id\"]\n",
                "                            user_id = json_node[\"user_id\"]\n",
                "                            business_id = json_node[\"business_id\"]\n",
                "                            text = json_node.get(\"text\", \"\")\n",
                "                            stars = json_node.get(\"stars\", 0)\n",
                "                            \n",
                "                            polarity = TextBlob(text).sentiment.polarity\n",
                "                            \n",
                "                            business_star_aggregates[business_id]['total_stars'] += stars\n",
                "                            business_star_aggregates[business_id]['review_count'] += 1\n",
                "                            \n",
                "                            writer.writerow({\n",
                "                                \"review_id:ID\": review_id,\n",
                "                                \"stars\": stars,\n",
                "                                \"date\": json_node.get(\"date\", \"\"),\n",
                "                                \"text\": text,\n",
                "                                \"useful:long\": json_node.get(\"useful\", 0),\n",
                "                                \"funny:long\": json_node.get(\"funny\", 0),\n",
                "                                \"cool:long\": json_node.get(\"cool\", 0),\n",
                "                                \"polarity:float\": polarity,\n",
                "                                \":LABEL\": REVIEW_NODE\n",
                "                            })\n",
                "                            \n",
                "                            wrote_relationships.add((user_id, review_id, WROTE))\n",
                "                            reviews_relationships.add((review_id, business_id, REVIEWS))\n",
                "                            processed_count += 1\n",
                "                        except json.JSONDecodeError:\n",
                "                            print(f\"Warning: Skipping invalid JSON line in {fixed_review_json_file}\")\n",
                "                        except KeyError as e:\n",
                "                            print(f\"Warning: Missing key {e} in review record: {line[:100]}...\")\n",
                "                            \n",
                "    except FileNotFoundError:\n",
                "        print(f\"Error: File not found {fixed_review_json_file}\")\n",
                "        raise\n",
                "    print(f\"Finished processing {processed_count} reviews and calculating polarity/aggregates.\")\n",
                "    return business_star_aggregates\n",
                "\n",
                "def process_tip_data():\n",
                "    \"\"\"Reads fixed tip data, populates tip nodes, WROTE and REVIEWS relationships, \n",
                "       only including tips where user and business exist.\"\"\"\n",
                "    print(f\"Processing tip data from {fixed_tip_json_file}...\")\n",
                "    global tip_wrote_relationships, tip_reviews_relationships\n",
                "    processed_count = 0\n",
                "    skipped_count = 0\n",
                "    \n",
                "    if not valid_user_ids or not valid_business_ids:\n",
                "        raise RuntimeError(\"Valid user and business ID sets are empty. Ensure process_user_data() and process_business_data() run first.\")\n",
                "        \n",
                "    try:\n",
                "        with open(fixed_tip_json_file, \"r\", encoding=\"utf-8\") as tjf:\n",
                "            with open(tip_nodes_csv_file, mode=\"w\", encoding=\"utf-8\", newline=\"\") as tip_csv:\n",
                "                fieldnames = [\"tip_id:ID\", \"text\", \"date\", \"compliment_count\", \":LABEL\"]\n",
                "                writer = csv.DictWriter(tip_csv, fieldnames=fieldnames, delimiter=\",\", quotechar='\"', quoting=csv.QUOTE_ALL)\n",
                "                writer.writeheader()\n",
                "                \n",
                "                for line in tjf:\n",
                "                    line = line.strip()\n",
                "                    if len(line) > 0:\n",
                "                        try:\n",
                "                            json_node = json.loads(line)\n",
                "                            tip_id = json_node[\"tip_id\"]\n",
                "                            user_id = json_node[\"user_id\"]\n",
                "                            business_id = json_node[\"business_id\"]\n",
                "                            \n",
                "                            if user_id in valid_user_ids and business_id in valid_business_ids:\n",
                "                                writer.writerow({\n",
                "                                    \"tip_id:ID\": tip_id,\n",
                "                                    \"text\": json_node.get(\"text\", \"\"),\n",
                "                                    \"date\": json_node.get(\"date\", \"\"),\n",
                "                                    \"compliment_count\": json_node.get(\"compliment_count\", 0),\n",
                "                                    \":LABEL\": TIP_NODE\n",
                "                                })\n",
                "                                \n",
                "                                tip_wrote_relationships.add((user_id, tip_id, WROTE))\n",
                "                                tip_reviews_relationships.add((tip_id, business_id, REVIEWS))\n",
                "                                processed_count += 1\n",
                "                            else:\n",
                "                                skipped_count += 1\n",
                "                                \n",
                "                        except json.JSONDecodeError:\n",
                "                            print(f\"Warning: Skipping invalid JSON line in {fixed_tip_json_file}\")\n",
                "                        except KeyError as e:\n",
                "                            print(f\"Warning: Missing key {e} in tip record: {line[:100]}...\")\n",
                "                            \n",
                "    except FileNotFoundError:\n",
                "        print(f\"Error: File not found {fixed_tip_json_file}\")\n",
                "        raise\n",
                "    print(f\"Finished processing tips. Added {processed_count} tips, skipped {skipped_count} due to missing user/business.\")\n",
                "\n",
                "def write_category_nodes_to_file():\n",
                "    \"\"\"Writes the collected unique category nodes to a CSV file.\"\"\"\n",
                "    print(f\"Writing {len(categories_nodes)} category nodes to {category_nodes_csv_file}...\")\n",
                "    with open(category_nodes_csv_file, mode=\"w\", encoding=\"utf-8\", newline=\"\") as category_csv:\n",
                "        fieldnames = [\"category_id:ID\", \":LABEL\"]\n",
                "        writer = csv.DictWriter(category_csv, fieldnames=fieldnames, delimiter=\",\", quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
                "        writer.writeheader()\n",
                "        for category_id in sorted(list(categories_nodes)):\n",
                "            writer.writerow({\"category_id:ID\": category_id, \":LABEL\": CATEGORY_NODE})\n",
                "    print(\"Finished writing category nodes.\")\n",
                "\n",
                "def write_location_nodes_to_file():\n",
                "    \"\"\"Writes the collected unique City, State, Country nodes to CSV files.\"\"\"\n",
                "    print(f\"Writing {len(city_nodes)} city nodes to {city_nodes_csv_file}...\")\n",
                "    with open(city_nodes_csv_file, mode=\"w\", encoding=\"utf-8\", newline=\"\") as city_csv:\n",
                "        fieldnames = [\"city_id:ID\", \"name\", \":LABEL\"]\n",
                "        writer = csv.DictWriter(city_csv, fieldnames=fieldnames, delimiter=\",\", quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
                "        writer.writeheader()\n",
                "        for city_id, city_name in sorted(list(city_nodes)):\n",
                "            writer.writerow({\"city_id:ID\": city_id, \"name\": city_name, \":LABEL\": CITY_NODE})\n",
                "            \n",
                "    print(f\"Writing {len(state_nodes)} state nodes to {state_nodes_csv_file}...\")\n",
                "    with open(state_nodes_csv_file, mode=\"w\", encoding=\"utf-8\", newline=\"\") as state_csv:\n",
                "        fieldnames = [\"state_id:ID\", \"name\", \":LABEL\"]\n",
                "        writer = csv.DictWriter(state_csv, fieldnames=fieldnames, delimiter=\",\", quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
                "        writer.writeheader()\n",
                "        for state_id, state_name in sorted(list(state_nodes)):\n",
                "            writer.writerow({\"state_id:ID\": state_id, \"name\": state_name, \":LABEL\": STATE_NODE})\n",
                "            \n",
                "    print(f\"Writing {len(country_nodes)} country nodes to {country_nodes_csv_file}...\")\n",
                "    with open(country_nodes_csv_file, mode=\"w\", encoding=\"utf-8\", newline=\"\") as country_csv:\n",
                "        fieldnames = [\"country_id:ID\", \":LABEL\"]\n",
                "        writer = csv.DictWriter(country_csv, fieldnames=fieldnames, delimiter=\",\", quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
                "        writer.writeheader()\n",
                "        for country_id in sorted(list(country_nodes)):\n",
                "            writer.writerow({\"country_id:ID\": country_id, \":LABEL\": COUNTRY_NODE})\n",
                "    print(\"Finished writing location nodes.\")\n",
                "\n",
                "def write_relationships_to_file():\n",
                "    \"\"\"Writes all collected relationships to a single CSV file.\"\"\"\n",
                "    all_relationships = (\n",
                "        in_category_relationships | in_city_relationships | \n",
                "        in_state_relationships | in_country_relationships | \n",
                "        friend_relationships | wrote_relationships | reviews_relationships | \n",
                "        tip_wrote_relationships | tip_reviews_relationships\n",
                "    )\n",
                "    print(f\"Writing {len(all_relationships)} relationships to {relationship_csv_file}...\")\n",
                "    with open(relationship_csv_file, mode=\"w\", encoding=\"utf-8\", newline=\"\") as rel_csv:\n",
                "        fieldnames = [\":START_ID\", \":END_ID\", \":TYPE\"]\n",
                "        writer = csv.DictWriter(rel_csv, fieldnames=fieldnames, delimiter=\",\", quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
                "        writer.writeheader()\n",
                "        for start_id, end_id, rel_type in sorted(list(all_relationships)):\n",
                "            writer.writerow({\":START_ID\": start_id, \":END_ID\": end_id, \":TYPE\": rel_type})\n",
                "    print(\"Finished writing relationships.\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Import CSV files not found or incomplete. Starting generation...\n",
                        "Processing user data from ../cleaned_yelp_data\\fixed_yelp_academic_dataset_user.json...\n",
                        "Finished processing 1677963 users. Found 1677963 valid user IDs.\n",
                        "Processing review data from ../cleaned_yelp_data\\fixed_yelp_academic_dataset_review.json...\n",
                        "Finished processing 5084370 reviews and calculating polarity/aggregates.\n",
                        "Processing business data from ../cleaned_yelp_data\\fixed_yelp_academic_dataset_business.json...\n",
                        "Finished processing 68696 businesses. Found 68696 valid business IDs.\n",
                        "Processing location data using reverse geocoding...\n",
                        "Performing reverse geocoding for 68696 coordinates...\n",
                        "Loading formatted geocoded file...\n",
                        "Processing geocoding results...\n",
                        "Finished processing locations: 620 cities, 14 states, 2 countries.\n",
                        "Processing tip data from ../cleaned_yelp_data\\fixed_yelp_academic_dataset_tip.json...\n",
                        "Finished processing tips. Added 741599 tips, skipped 0 due to missing user/business.\n",
                        "Writing 878 category nodes to category_nodes.csv...\n",
                        "Finished writing category nodes.\n",
                        "Writing 620 city nodes to city_nodes.csv...\n",
                        "Writing 14 state nodes to state_nodes.csv...\n",
                        "Writing 2 country nodes to country_nodes.csv...\n",
                        "Finished writing location nodes.\n",
                        "Writing 19333704 relationships to relationships.csv...\n",
                        "Finished writing relationships.\n",
                        "CSV file generation complete.\n"
                    ]
                }
            ],
            "source": [
                "# --- CSV Generation Execution ---\n",
                "\n",
                "# Check if CSV files already exist. If not, generate them.\n",
                "if not all(Path(f).is_file() for f in nodes_files) or not Path(relationship_csv_file).is_file():\n",
                "    print(\"Import CSV files not found or incomplete. Starting generation...\")\n",
                "    # Delete any existing CSV files first\n",
                "    delete_files(nodes_files)\n",
                "    delete_files([relationship_csv_file])\n",
                "    \n",
                "    # Process data and populate node/relationship sets\n",
                "    process_user_data() \n",
                "    business_star_aggregates = process_review_data_with_polarity_and_aggregation() \n",
                "    process_business_data(business_star_aggregates) \n",
                "    process_location_data() \n",
                "    process_tip_data() \n",
                "    \n",
                "    # Write node sets to their respective CSV files\n",
                "    write_category_nodes_to_file()\n",
                "    write_location_nodes_to_file()\n",
                "    \n",
                "    # Write all relationships to the relationship CSV file\n",
                "    write_relationships_to_file()\n",
                "    \n",
                "    # Optional: Clean up large intermediate sets to free memory\n",
                "    del business_lat_lon, categories_nodes, city_nodes, state_nodes, country_nodes\n",
                "    del valid_user_ids, valid_business_ids\n",
                "    del in_category_relationships, in_city_relationships, in_state_relationships, in_country_relationships\n",
                "    del friend_relationships, wrote_relationships, reviews_relationships\n",
                "    del tip_wrote_relationships, tip_reviews_relationships\n",
                "    del business_star_aggregates\n",
                "    \n",
                "    print(\"CSV file generation complete.\")\n",
                "else:\n",
                "    print(\"Import CSV files already exist. Skipping generation.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Data Integrity Check\n",
                "\n",
                "Verify basic integrity of the generated CSV files before attempting import:\n",
                "- Check for duplicate IDs within each node file.\n",
                "- Check for duplicate relationships (same start, end, and type)."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Checking integrity of generated CSV files...\n",
                        "  Checking node file: business_nodes.csv...\n",
                        "    Found 68696 unique nodes.\n",
                        "  Checking node file: category_nodes.csv...\n",
                        "    Found 878 unique nodes.\n",
                        "  Checking node file: city_nodes.csv...\n",
                        "    Found 620 unique nodes.\n",
                        "  Checking node file: state_nodes.csv...\n",
                        "    Found 14 unique nodes.\n",
                        "  Checking node file: country_nodes.csv...\n",
                        "    Found 2 unique nodes.\n",
                        "  Checking node file: user_nodes.csv...\n",
                        "    Found 1677963 unique nodes.\n",
                        "  Checking node file: review_nodes.csv...\n",
                        "    Found 5084370 unique nodes.\n",
                        "  Checking node file: tip_nodes.csv...\n",
                        "    Found 741599 unique nodes.\n",
                        "  Checking relationship file: relationships.csv...\n",
                        "    Found 19333704 unique relationships.\n",
                        "Integrity check passed. Total unique nodes expected: 7574142\n"
                    ]
                }
            ],
            "source": [
                "num_nodes_total = 0\n",
                "\n",
                "def check_nodes_relationships_csv_files_integrity():\n",
                "    \"\"\"Validates uniqueness of node IDs and relationships in generated CSVs.\"\"\"\n",
                "    global num_nodes_total\n",
                "    num_nodes_total = 0 # Reset count\n",
                "    print(\"Checking integrity of generated CSV files...\")\n",
                "    \n",
                "    # Check Node Files\n",
                "    for one_node_file in nodes_files:\n",
                "        print(f\"  Checking node file: {one_node_file}...\")\n",
                "        try:\n",
                "            if not Path(one_node_file).is_file():\n",
                "                 print(f\"    Warning: Node file {one_node_file} not found during integrity check. Skipping.\")\n",
                "                 continue\n",
                "            temp_df = pd.read_csv(one_node_file, header=\"infer\", sep=\",\", encoding=\"utf-8\", low_memory=False)\n",
                "            if temp_df.empty:\n",
                "                 print(f\"    Warning: Node file {one_node_file} is empty. Skipping ID check.\")\n",
                "                 continue\n",
                "            ids = temp_df.iloc[:, 0]\n",
                "            if len(ids) != len(ids.unique()):\n",
                "                duplicates = ids[ids.duplicated()].unique()\n",
                "                raise ValueError(f\"Duplicate node IDs found in [{one_node_file}]: {list(duplicates)[:5]}...\")\n",
                "            num_nodes_total += len(ids)\n",
                "            print(f\"    Found {len(ids)} unique nodes.\")\n",
                "        except FileNotFoundError:\n",
                "            print(f\"Error: Node file not found: {one_node_file}\")\n",
                "            raise\n",
                "        except Exception as e:\n",
                "            print(f\"Error checking node file {one_node_file}: {e}\")\n",
                "            raise\n",
                "            \n",
                "    # Check Relationship File\n",
                "    print(f\"  Checking relationship file: {relationship_csv_file}...\")\n",
                "    try:\n",
                "        if not Path(relationship_csv_file).is_file():\n",
                "            print(f\"    Warning: Relationship file {relationship_csv_file} not found during integrity check. Skipping.\")\n",
                "        else:\n",
                "            temp_df = pd.read_csv(relationship_csv_file, header=\"infer\", sep=\",\", encoding=\"utf-8\", low_memory=False)\n",
                "            if temp_df.empty:\n",
                "                 print(f\"    Warning: Relationship file {relationship_csv_file} is empty. Skipping relationship check.\")\n",
                "            else:\n",
                "                rel_signatures = temp_df.iloc[:, 0].astype(str) + \"|\" + temp_df.iloc[:, 1].astype(str) + \"|\" + temp_df.iloc[:, 2].astype(str)\n",
                "                if len(rel_signatures) != len(rel_signatures.unique()):\n",
                "                    duplicates = rel_signatures[rel_signatures.duplicated()].unique()\n",
                "                    raise ValueError(f\"Duplicate relationships found in [{relationship_csv_file}]: {list(duplicates)[:5]}...\")\n",
                "                print(f\"    Found {len(rel_signatures)} unique relationships.\")\n",
                "    except FileNotFoundError:\n",
                "        print(f\"Error: Relationship file not found: {relationship_csv_file}\")\n",
                "        raise\n",
                "    except Exception as e:\n",
                "        print(f\"Error checking relationship file {relationship_csv_file}: {e}\")\n",
                "        raise\n",
                "        \n",
                "    print(f\"Integrity check passed. Total unique nodes expected: {num_nodes_total}\")\n",
                "\n",
                "# Run the check\n",
                "try:\n",
                "    check_nodes_relationships_csv_files_integrity()\n",
                "except Exception as e:\n",
                "    print(f\"\\n--- DATA INTEGRITY CHECK FAILED ---: {e}\")\n",
                "    print(\"Import aborted. Please check the CSV generation process and input data.\")\n",
                "    # raise"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6. Importing Data into Neo4j\n",
                "\n",
                "This section uses the `neo4j-admin import` tool to load the generated CSV files into the Neo4j database. \n",
                "\n",
                "**Process:**\n",
                "1.  Stop the Neo4j Database Service.\n",
                "2.  Delete the existing database files (effectively resetting the graph).\n",
                "3.  Run `neo4j-admin import` using the generated CSV files.\n",
                "4.  Start the Neo4j Database Service.\n",
                "5.  Verify the import by checking the node count.\n",
                "\n",
                "**Important Notes:**\n",
                "- Requires `neo4j_home` to be correctly identified earlier.\n",
                "- Requires sufficient permissions to stop/start the Neo4j service and modify its data directories.\n",
                "- This will **completely overwrite** the target Neo4j database (`graph_name`)."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 9,
            "metadata": {},
            "outputs": [],
            "source": [
                "# --- Neo4j Import Functions ---\n",
                "\n",
                "def command_neo4j_database_service(cmd: str):\n",
                "    \"\"\"Executes start/stop commands for the Neo4j service.\"\"\"\n",
                "    if neo4j_home is None or not Path(neo4j_home).is_dir():\n",
                "        raise ValueError(\"Neo4j home directory ('neo4j_home') is not set or invalid. Please configure it in cell [2].\")\n",
                "        \n",
                "    neo4j_cmd_path = os.path.join(neo4j_home, \"bin\", \"neo4j.bat\") # Assuming Windows .bat\n",
                "    if not Path(neo4j_cmd_path).is_file():\n",
                "         neo4j_cmd_path = os.path.join(neo4j_home, \"bin\", \"neo4j\") # Try Linux/macOS script\n",
                "         if not Path(neo4j_cmd_path).is_file():\n",
                "             raise FileNotFoundError(f\"Neo4j command script not found in {os.path.join(neo4j_home, 'bin')}\")\n",
                "             \n",
                "    if cmd in [\"stop\", \"start\"]:\n",
                "        print(f\"Attempting to {cmd} Neo4j service...\")\n",
                "        try:\n",
                "            cmd_res = subprocess.run([neo4j_cmd_path, cmd], capture_output=True, text=True, check=False, shell=True)\n",
                "            \n",
                "            print(f\"  STDOUT: {cmd_res.stdout.strip()}\")\n",
                "            print(f\"  STDERR: {cmd_res.stderr.strip()}\")\n",
                "            \n",
                "            if cmd == \"stop\":\n",
                "                if \"stopped\" not in cmd_res.stdout.lower() and \"not running\" not in cmd_res.stderr.lower():\n",
                "                     print(f\"Warning: Neo4j stop command finished, but output doesn't confirm stop. Check service status manually.\")\n",
                "                else:\n",
                "                    print(\"Neo4j service appears stopped.\")\n",
                "                    sleep(5)\n",
                "            elif cmd == \"start\":\n",
                "                if \"started\" not in cmd_res.stdout.lower():\n",
                "                    raise RuntimeError(f\"Failed to start Neo4j service. Check logs. STDERR: {cmd_res.stderr.strip()}\")\n",
                "                else:\n",
                "                    print(\"Neo4j service appears started. Waiting for it to become available...\")\n",
                "                    sleep(15)\n",
                "                    \n",
                "        except FileNotFoundError:\n",
                "            raise FileNotFoundError(f\"Neo4j command '{neo4j_cmd_path}' not found.\")\n",
                "        except subprocess.CalledProcessError as e:\n",
                "            raise RuntimeError(f\"Error executing Neo4j {cmd} command: {e}. STDOUT: {e.stdout}. STDERR: {e.stderr}\")\n",
                "        except Exception as e:\n",
                "            raise RuntimeError(f\"An unexpected error occurred during Neo4j {cmd}: {e}\")\n",
                "    else:\n",
                "        raise ValueError(f'Unknown command for Neo4j service: [{cmd}]')\n",
                "\n",
                "def reset_neo4j_database():\n",
                "    \"\"\"Deletes the data directories for the target graph.\"\"\"\n",
                "    if neo4j_home is None or not Path(neo4j_home).is_dir():\n",
                "        raise ValueError(\"Neo4j home directory ('neo4j_home') is not set or invalid. Please configure it in cell [2].\")\n",
                "        \n",
                "    neo4j_db_dir = Path(neo4j_home) / \"data\" / \"databases\" / graph_name\n",
                "    neo4j_tx_dir = Path(neo4j_home) / \"data\" / \"transactions\" / graph_name\n",
                "    \n",
                "    print(f\"Attempting to delete database directory: {neo4j_db_dir}\")\n",
                "    if neo4j_db_dir.exists() and neo4j_db_dir.is_dir():\n",
                "        try:\n",
                "            shutil.rmtree(neo4j_db_dir)\n",
                "            print(\"  Database directory deleted.\")\n",
                "        except OSError as e:\n",
                "            raise OSError(f\"Error deleting database directory {neo4j_db_dir}: {e}. Check permissions and ensure Neo4j is stopped.\")\n",
                "    else:\n",
                "        print(\"  Database directory does not exist, skipping deletion.\")\n",
                "        \n",
                "    print(f\"Attempting to delete transactions directory: {neo4j_tx_dir}\")\n",
                "    if neo4j_tx_dir.exists() and neo4j_tx_dir.is_dir():\n",
                "        try:\n",
                "            shutil.rmtree(neo4j_tx_dir)\n",
                "            print(\"  Transactions directory deleted.\")\n",
                "        except OSError as e:\n",
                "            print(f\"Warning: Could not delete transactions directory {neo4j_tx_dir}: {e}. Import might still succeed.\")\n",
                "    else:\n",
                "        print(\"  Transactions directory does not exist, skipping deletion.\")\n",
                "\n",
                "import os\n",
                "import subprocess\n",
                "from pathlib import Path\n",
                "\n",
                "def import_data():\n",
                "    if not neo4j_home or not Path(neo4j_home).is_dir():\n",
                "         raise ValueError(f\"Neo4j home directory ('neo4j_home') is not set or invalid: {neo4j_home}\")\n",
                "\n",
                "    import_tool = Path(neo4j_home) / \"bin\" / (\"neo4j-admin.bat\" if os.name == \"nt\" else \"neo4j-admin\")\n",
                "    if not import_tool.is_file():\n",
                "        raise FileNotFoundError(f\"neo4j-admin executable not found at: {import_tool}\")\n",
                "\n",
                "    csv_dir = Path.cwd()\n",
                "\n",
                "    args = [\n",
                "        str(import_tool),\n",
                "        \"database\", \"import\", \"full\",\n",
                "        \"--overwrite-destination=true\",\n",
                "        \"--multiline-fields=true\",\n",
                "        \"--skip-duplicate-nodes=true\",\n",
                "        \"--skip-bad-relationships=true\",\n",
                "    ]\n",
                "\n",
                "    for f in nodes_files:\n",
                "        node_file_path = (csv_dir / f).resolve()\n",
                "        if node_file_path.is_file():\n",
                "             args.append(f\"--nodes={node_file_path}\")\n",
                "        else:\n",
                "             print(f\"Warning: Node file not found, skipping: {node_file_path}\")\n",
                "\n",
                "    rel_file_path = (csv_dir / relationship_csv_file).resolve()\n",
                "    if rel_file_path.is_file():\n",
                "        args.append(f\"--relationships={rel_file_path}\")\n",
                "    else:\n",
                "        print(f\"Warning: Relationship file not found, skipping: {rel_file_path}\")\n",
                "\n",
                "    # Add the database name as the VERY LAST positional argument\n",
                "    # args.append(graph_name) # This seems incorrect for neo4j-admin 5.x, handled by default or config\n",
                "\n",
                "    print(\"Running:\", \" \".join(map(str, args)))\n",
                "\n",
                "    env = os.environ.copy()\n",
                "    java_home_path = \"C:/Program Files/Eclipse Adoptium/jdk-21.0.7.6-hotspot\"\n",
                "    if Path(java_home_path).is_dir():\n",
                "         env[\"JAVA_HOME\"] = java_home_path\n",
                "         print(f\"Setting JAVA_HOME to: {java_home_path}\")\n",
                "    else:\n",
                "         print(f\"Warning: Specified JAVA_HOME path not found: {java_home_path}. neo4j-admin might fail.\")\n",
                "\n",
                "    try:\n",
                "        res = subprocess.run(args, text=True, capture_output=True, env=env, check=False, shell=False)\n",
                "\n",
                "        if res.returncode != 0:\n",
                "            print(\"\\n--- FAILED COMMAND ---\")\n",
                "            print(\" \".join(map(str, args)))\n",
                "            print(\"----------------------\\n\")\n",
                "            raise RuntimeError(f\"Import failed ({res.returncode})\\nSTDOUT:\\n{res.stdout}\\nSTDERR:\\n{res.stderr}\")\n",
                "        else:\n",
                "             print(\"\\n--- Import Command Output ---\")\n",
                "             print(res.stdout)\n",
                "             print(\"---------------------------\\n\")\n",
                "             if res.stderr:\n",
                "                 print(\"--- Import Command STDERR (Warnings/Info) ---\")\n",
                "                 print(res.stderr)\n",
                "                 print(\"---------------------------------------------\\n\")\n",
                "\n",
                "    except FileNotFoundError as fnf_error:\n",
                "         print(f\"Error: Command not found. Ensure neo4j-admin path is correct. Details: {fnf_error}\")\n",
                "         raise\n",
                "    except Exception as e:\n",
                "         print(f\"An unexpected error occurred running the import command: {e}\")\n",
                "         raise"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 10,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Checking current graph 'neo4j' node count...\n",
                        "An unexpected error occurred checking node count: Cannot open connection to ConnectionProfile('bolt://localhost:7687'). Assuming import is needed.\n",
                        "\n",
                        "--- Starting Neo4j Import Process ---\n",
                        "Attempting to delete database directory: C:\\Users\\wiztu\\.Neo4jDesktop\\relate-data\\dbmss\\dbms-d9cecb50-c260-4dd0-b3e0-6b387c58d5ec\\data\\databases\\neo4j\n",
                        "  Database directory deleted.\n",
                        "Attempting to delete transactions directory: C:\\Users\\wiztu\\.Neo4jDesktop\\relate-data\\dbmss\\dbms-d9cecb50-c260-4dd0-b3e0-6b387c58d5ec\\data\\transactions\\neo4j\n",
                        "  Transactions directory deleted.\n",
                        "Running: C:\\Users\\wiztu\\.Neo4jDesktop\\relate-data\\dbmss\\dbms-d9cecb50-c260-4dd0-b3e0-6b387c58d5ec\\bin\\neo4j-admin.bat database import full --overwrite-destination=true --multiline-fields=true --skip-duplicate-nodes=true --skip-bad-relationships=true --nodes=C:\\Users\\wiztu\\OneDrive\\Documents\\Data Science and Management\\Project\\scripts\\business_nodes.csv --nodes=C:\\Users\\wiztu\\OneDrive\\Documents\\Data Science and Management\\Project\\scripts\\category_nodes.csv --nodes=C:\\Users\\wiztu\\OneDrive\\Documents\\Data Science and Management\\Project\\scripts\\city_nodes.csv --nodes=C:\\Users\\wiztu\\OneDrive\\Documents\\Data Science and Management\\Project\\scripts\\state_nodes.csv --nodes=C:\\Users\\wiztu\\OneDrive\\Documents\\Data Science and Management\\Project\\scripts\\country_nodes.csv --nodes=C:\\Users\\wiztu\\OneDrive\\Documents\\Data Science and Management\\Project\\scripts\\user_nodes.csv --nodes=C:\\Users\\wiztu\\OneDrive\\Documents\\Data Science and Management\\Project\\scripts\\review_nodes.csv --nodes=C:\\Users\\wiztu\\OneDrive\\Documents\\Data Science and Management\\Project\\scripts\\tip_nodes.csv --relationships=C:\\Users\\wiztu\\OneDrive\\Documents\\Data Science and Management\\Project\\scripts\\relationships.csv\n",
                        "Setting JAVA_HOME to: C:/Program Files/Eclipse Adoptium/jdk-21.0.7.6-hotspot\n",
                        "\n",
                        "--- Import Command Output ---\n",
                        "Neo4j version: 5.24.0\n",
                        "Importing the contents of these files into C:\\Users\\wiztu\\.Neo4jDesktop\\relate-data\\dbmss\\dbms-d9cecb50-c260-4dd0-b3e0-6b387c58d5ec\\data\\databases\\neo4j:\n",
                        "Nodes:\n",
                        "  C:\\Users\\wiztu\\OneDrive\\Documents\\Data Science and Management\\Project\\scripts\\business_nodes.csv\n",
                        "  C:\\Users\\wiztu\\OneDrive\\Documents\\Data Science and Management\\Project\\scripts\\category_nodes.csv\n",
                        "  C:\\Users\\wiztu\\OneDrive\\Documents\\Data Science and Management\\Project\\scripts\\city_nodes.csv\n",
                        "  C:\\Users\\wiztu\\OneDrive\\Documents\\Data Science and Management\\Project\\scripts\\state_nodes.csv\n",
                        "  C:\\Users\\wiztu\\OneDrive\\Documents\\Data Science and Management\\Project\\scripts\\country_nodes.csv\n",
                        "  C:\\Users\\wiztu\\OneDrive\\Documents\\Data Science and Management\\Project\\scripts\\user_nodes.csv\n",
                        "  C:\\Users\\wiztu\\OneDrive\\Documents\\Data Science and Management\\Project\\scripts\\review_nodes.csv\n",
                        "  C:\\Users\\wiztu\\OneDrive\\Documents\\Data Science and Management\\Project\\scripts\\tip_nodes.csv\n",
                        "\n",
                        "Relationships:\n",
                        "  C:\\Users\\wiztu\\OneDrive\\Documents\\Data Science and Management\\Project\\scripts\\relationships.csv\n",
                        "\n",
                        "\n",
                        "Available resources:\n",
                        "  Total machine memory: 15.84GiB\n",
                        "  Free machine memory: 8.397GiB\n",
                        "  Max heap memory : 910.5MiB\n",
                        "  Max worker threads: 8\n",
                        "  Configured max memory: 6.911GiB\n",
                        "  High parallel IO: true\n",
                        "\n",
                        "Cypher type normalization is enabled (disable with --normalize-types=false):\n",
                        "  Property type of 'avgStar' normalized from 'float' --> 'double' in C:\\Users\\wiztu\\OneDrive\\Documents\\Data Science and Management\\Project\\scripts\\business_nodes.csv\n",
                        "  Property type of 'polarity' normalized from 'float' --> 'double' in C:\\Users\\wiztu\\OneDrive\\Documents\\Data Science and Management\\Project\\scripts\\review_nodes.csv\n",
                        "Import starting\n",
                        "  Page cache size: 1.992GiB\n",
                        "  Number of worker threads: 8\n",
                        "  Estimated number of nodes: 7.631.008\n",
                        "  Estimated number of relationships: 22.680.988\n",
                        "\n",
                        "Importing nodes\n",
                        ".......... .......... .......... .......... ..........   5% âˆ†2s 177ms [2s 177ms]\n",
                        ".......... .......... .......... .......... ..........  10% âˆ†417ms [2s 595ms]\n",
                        ".......... .......... .......... .......... ..........  15% âˆ†376ms [2s 971ms]\n",
                        ".......... .......... .......... .......... ..........  20% âˆ†423ms [3s 394ms]\n",
                        ".......... .......... .......... .......... ..........  25% âˆ†1s 321ms [4s 716ms]\n",
                        ".......... .......... .......... .......... ..........  30% âˆ†2s 147ms [6s 863ms]\n",
                        ".......... .......... .......... .......... ..........  35% âˆ†1s 742ms [8s 605ms]\n",
                        ".......... .......... .......... .......... ..........  40% âˆ†1s 605ms [10s 211ms]\n",
                        ".......... .......... .......... .......... ..........  45% âˆ†1s 694ms [11s 905ms]\n",
                        ".......... .......... .......... .......... ..........  50% âˆ†4s 945ms [16s 851ms]\n",
                        ".......... .......... .......... .......... ..........  55% âˆ†3s 830ms [20s 682ms]\n",
                        ".......... .......... .......... .......... ..........  60% âˆ†4s 665ms [25s 347ms]\n",
                        ".......... .......... .......... .......... ..........  65% âˆ†5s 241ms [30s 588ms]\n",
                        ".......... .......... .......... .......... ..........  70% âˆ†8s 812ms [39s 401ms]\n",
                        ".......... .......... .......... .......... ..........  75% âˆ†10s 92ms [49s 493ms]\n",
                        ".......... .......... .......... .......... ..........  80% âˆ†5s 702ms [55s 196ms]\n",
                        ".......... .......... .......... .......... ..........  85% âˆ†3s 351ms [58s 547ms]\n",
                        ".......... .......... .......... .......... ..........  90% âˆ†4s 818ms [1m 3s 366ms]\n",
                        ".......... .......... .......... .......... ..........  95% âˆ†4s 82ms [1m 7s 448ms]\n",
                        ".......... .......... .......... .......... .......... 100% âˆ†1s 734ms [1m 9s 183ms]\n",
                        "Imported 7,574,142 nodes in 1m 9s 190ms\n",
                        "Prepare ID mapper\n",
                        ".......... .......... .......... .......... ..........   5% âˆ†66ms [66ms]\n",
                        ".......... .......... .......... .......... ..........  10% âˆ†26ms [93ms]\n",
                        ".......... .......... .......... .......... ..........  15% âˆ†26ms [119ms]\n",
                        ".......... .......... .......... .......... ..........  20% âˆ†26ms [146ms]\n",
                        ".......... .......... .......... .......... ..........  25% âˆ†26ms [172ms]\n",
                        ".......... .......... .......... .......... ..........  30% âˆ†26ms [199ms]\n",
                        ".......... .......... .......... .......... ..........  35% âˆ†530ms [729ms]\n",
                        ".......... .......... .......... .......... ..........  40% âˆ†132ms [861ms]\n",
                        ".......... .......... .......... .......... ..........  45% âˆ†123ms [985ms]\n",
                        ".......... .......... .......... .......... ..........  50% âˆ†120ms [1s 105ms]\n",
                        ".......... .......... .......... .......... ..........  55% âˆ†148ms [1s 254ms]\n",
                        ".......... .......... .......... .......... ..........  60% âˆ†128ms [1s 382ms]\n",
                        ".......... .......... .......... .......... ..........  65% âˆ†127ms [1s 510ms]\n",
                        ".......... .......... .......... .......... ..........  70% âˆ†132ms [1s 642ms]\n",
                        ".......... .......... .......... .......... ..........  75% âˆ†107ms [1s 749ms]\n",
                        ".......... .......... .......... .......... ..........  80% âˆ†82ms [1s 832ms]\n",
                        ".......... .......... .......... .......... ..........  85% âˆ†18ms [1s 850ms]\n",
                        ".......... .......... .......... .......... ..........  90% âˆ†18ms [1s 869ms]\n",
                        ".......... .......... .......... .......... ..........  95% âˆ†18ms [1s 887ms]\n",
                        ".......... .......... .......... .......... .......... 100% âˆ†18ms [1s 906ms]\n",
                        "Importing relationships\n",
                        "Converting to intermediary format\n",
                        ".......... .......... .......... .......... ..........   5% âˆ†1s 82ms [1s 82ms]\n",
                        ".......... .......... .......... .......... ..........  10% âˆ†788ms [1s 871ms]\n",
                        ".......... .......... .......... .......... ..........  15% âˆ†1s 81ms [2s 952ms]\n",
                        ".......... .......... .......... .......... ..........  20% âˆ†1s 57ms [4s 10ms]\n",
                        ".......... .......... .......... .......... ..........  25% âˆ†1s 123ms [5s 133ms]\n",
                        ".......... .......... .......... .......... ..........  30% âˆ†1s 733ms [6s 866ms]\n",
                        ".......... .......... .......... .......... ..........  35% âˆ†816ms [7s 683ms]\n",
                        ".......... .......... .......... .......... ..........  40% âˆ†934ms [8s 617ms]\n",
                        ".......... .......... .......... .......... ..........  45% âˆ†877ms [9s 495ms]\n",
                        ".......... .......... .......... .......... ..........  50% âˆ†859ms [10s 354ms]\n",
                        ".......... .......... .......... .......... ..........  55% âˆ†799ms [11s 154ms]\n",
                        ".......... .......... .......... .......... ..........  60% âˆ†944ms [12s 98ms]\n",
                        ".......... .......... .......... .......... ..........  65% âˆ†897ms [12s 996ms]\n",
                        ".......... .......... .......... .......... ..........  70% âˆ†1s 28ms [14s 25ms]\n",
                        ".......... .......... .......... .......... ..........  75% âˆ†958ms [14s 983ms]\n",
                        ".......... .......... .......... .......... ..........  80% âˆ†1s 107ms [16s 91ms]\n",
                        ".......... .......... .......... .......... ..........  85% âˆ†842ms [16s 934ms]\n",
                        ".......... .......... .......... .......... ..........  90% âˆ†164ms [17s 98ms]\n",
                        ".......... .......... .......... .......... ..........  95% âˆ†0ms [17s 98ms]\n",
                        ".......... .......... .......... .......... .......... 100% âˆ†0ms [17s 98ms]\n",
                        "  using configuration:Configuration[numberOfWorkers=8, temporaryPath=C:\\Users\\wiztu\\.Neo4jDesktop\\relate-data\\dbmss\\dbms-d9cecb50-c260-4dd0-b3e0-6b387c58d5ec\\data\\databases\\neo4j\\temp, numberOfTrackedDense=10000, applyBatchSize=64]\n",
                        "Importing relationships\n",
                        ".......... .......... .......... .......... ..........   5% âˆ†1s 105ms [1s 105ms]\n",
                        ".......... .......... .......... .......... ..........  10% âˆ†562ms [1s 667ms]\n",
                        ".......... .......... .......... .......... ..........  15% âˆ†437ms [2s 105ms]\n",
                        ".......... .......... .......... .......... ..........  20% âˆ†383ms [2s 488ms]\n",
                        ".......... .......... .......... .......... ..........  25% âˆ†469ms [2s 958ms]\n",
                        ".......... .......... .......... .......... ..........  30% âˆ†327ms [3s 285ms]\n",
                        ".......... .......... .......... .......... ..........  35% âˆ†444ms [3s 730ms]\n",
                        ".......... .......... .......... .......... ..........  40% âˆ†1s 26ms [4s 757ms]\n",
                        ".......... .......... .......... .......... ..........  45% âˆ†1s 390ms [6s 147ms]\n",
                        ".......... .......... .......... .......... ..........  50% âˆ†818ms [6s 965ms]\n",
                        ".......... .......... .......... .......... ..........  55% âˆ†970ms [7s 935ms]\n",
                        ".......... .......... .......... .......... ..........  60% âˆ†1s 273ms [9s 209ms]\n",
                        ".......... .......... .......... .......... ..........  65% âˆ†2s 252ms [11s 461ms]\n",
                        ".......... .......... .......... .......... ..........  70% âˆ†18s 855ms [30s 317ms]\n",
                        ".......... .......... .......... .......... ..........  75% âˆ†231ms [30s 549ms]\n",
                        ".......... .......... .......... .......... ..........  80% âˆ†4s 77ms [34s 626ms]\n",
                        ".......... .......... .......... .......... ..........  85% âˆ†947ms [35s 574ms]\n",
                        ".......... .......... .......... .......... ..........  90% âˆ†57ms [35s 631ms]\n",
                        ".......... .......... .......... .......... ..........  95% âˆ†0ms [35s 631ms]\n",
                        ".......... .......... .......... .......... .......... 100% âˆ†0ms [35s 631ms]\n",
                        "Imported 26,907,507 relationships in 53s 839ms\n",
                        "Flushing stores\n",
                        "Flush completed in 5s 956ms\n",
                        "IMPORT DONE in 2m 12s 85ms.\n",
                        "\n",
                        "---------------------------\n",
                        "\n",
                        "--- Neo4j Import Process Finished ---\n"
                    ]
                }
            ],
            "source": [
                "# --- Import Execution ---\n",
                "\n",
                "run_import = False\n",
                "try:\n",
                "    if 'num_nodes_total' not in globals() or num_nodes_total == 0:\n",
                "         print(\"Warning: Expected node count not calculated. Running integrity check first.\")\n",
                "         check_nodes_relationships_csv_files_integrity()\n",
                "         \n",
                "    print(f\"Checking current graph '{graph_name}' node count...\")\n",
                "    graph = Graph(SERVER_ADDRESS, auth=SERVER_AUTH)\n",
                "    current_node_count = graph.nodes.match().count()\n",
                "    print(f\"  Current node count: {current_node_count}\")\n",
                "    print(f\"  Expected node count from CSVs: {num_nodes_total}\")\n",
                "    \n",
                "    if current_node_count != num_nodes_total or num_nodes_total == 0:\n",
                "        print(\"Node count mismatch or expected count is zero. Proceeding with import.\")\n",
                "        run_import = True\n",
                "    else:\n",
                "        print(\"Node count matches expected count. Skipping import.\")\n",
                "        \n",
                "except (ServiceUnavailable, TransientError, Neo4jError, OSError) as e:\n",
                "    print(f\"Could not connect to Neo4j to check node count ({e}). Assuming import is needed.\")\n",
                "    run_import = True\n",
                "except NameError:\n",
                "     print(\"Expected node count variable 'num_nodes_total' not defined. Assuming import is needed.\")\n",
                "     run_import = True\n",
                "except Exception as e:\n",
                "    print(f\"An unexpected error occurred checking node count: {e}. Assuming import is needed.\")\n",
                "    run_import = True\n",
                "\n",
                "if run_import:\n",
                "    try:\n",
                "        print(\"\\n--- Starting Neo4j Import Process ---\")\n",
                "        #command_neo4j_database_service(\"stop\")\n",
                "        reset_neo4j_database()\n",
                "        import_data()\n",
                "        #command_neo4j_database_service(\"start\")\n",
                "        print(\"--- Neo4j Import Process Finished ---\")\n",
                "    except Exception as e:\n",
                "        print(f\"\\n--- IMPORT PROCESS FAILED ---: {e}\")\n",
                "        print(\"Attempting to start Neo4j service if it was stopped...\")\n",
                "        try:\n",
                "            #command_neo4j_database_service(\"start\") \n",
                "            print(\"Neo4j service start command skipped after failed import.\")\n",
                "        except Exception as start_err:\n",
                "            print(f\"Could not restart Neo4j service after failed import: {start_err}\")\n",
                "        # raise e \n",
                "else:\n",
                "    print(\"\\nImport skipped as node count matches.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 7. Final Verification\n",
                "\n",
                "Attempt to connect to the Neo4j database and verify the node count again after the import process."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 11,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "Verifying import results (expected nodes: 7574142)...\n",
                        "Attempt 1/30: Connecting to Neo4j...\n",
                        "  An unexpected error occurred during verification: Cannot open connection to ConnectionProfile('bolt://localhost:7687')\n",
                        "\n",
                        "Verification failed after 30 attempts. Could not confirm successful import.\n",
                        "Last recorded node count: -1\n",
                        "\n",
                        "--- FINAL VERIFICATION FAILED ---\n"
                    ]
                }
            ],
            "source": [
                "yelp_graph_ready = False\n",
                "final_node_count = -1\n",
                "\n",
                "def check_if_importing_is_successful():\n",
                "    \"\"\"Tries to connect to Neo4j and checks if the node count matches the expected count.\"\"\"\n",
                "    global graph, yelp_graph_ready, final_node_count\n",
                "    \n",
                "    if 'num_nodes_total' not in globals() or num_nodes_total == 0:\n",
                "        try:\n",
                "             print(\"Recalculating expected node count for final verification...\")\n",
                "             check_nodes_relationships_csv_files_integrity()\n",
                "        except Exception as e:\n",
                "             print(f\"Error recalculating expected node count: {e}. Cannot verify import accurately.\")\n",
                "             return\n",
                "             \n",
                "    print(f\"\\nVerifying import results (expected nodes: {num_nodes_total})...\")\n",
                "    num_tries = 30\n",
                "    wait_interval = 2\n",
                "    \n",
                "    for one_try in range(num_tries):\n",
                "        try:\n",
                "            print(f\"Attempt {one_try + 1}/{num_tries}: Connecting to Neo4j...\")\n",
                "            graph = Graph(SERVER_ADDRESS, auth=SERVER_AUTH)\n",
                "            final_node_count = graph.nodes.match().count()\n",
                "            print(f\"  Successfully connected. Found {final_node_count} nodes.\")\n",
                "            \n",
                "            if final_node_count == 0 and num_nodes_total > 0:\n",
                "                print(\"  Warning: Graph is empty, but expected nodes. Import might have failed silently or service is still starting.\")\n",
                "            elif final_node_count != num_nodes_total:\n",
                "                print(f\"  Warning: Node count mismatch! Expected {num_nodes_total}, found {final_node_count}.\")\n",
                "                yelp_graph_ready = True\n",
                "                return\n",
                "            else:\n",
                "                print(\"  Node count matches expected count. Import successful!\")\n",
                "                yelp_graph_ready = True\n",
                "                return\n",
                "                \n",
                "        except (ServiceUnavailable, TransientError, Neo4jError, OSError) as e:\n",
                "            print(f\"  Connection failed ({e}). Neo4j might still be starting. Retrying in {wait_interval}s...\")\n",
                "        except Exception as e:\n",
                "            print(f\"  An unexpected error occurred during verification: {e}\")\n",
                "            break \n",
                "            \n",
                "        if not yelp_graph_ready:\n",
                "             sleep(wait_interval)\n",
                "             \n",
                "    if not yelp_graph_ready:\n",
                "        print(f\"\\nVerification failed after {num_tries} attempts. Could not confirm successful import.\")\n",
                "        print(f\"Last recorded node count: {final_node_count}\")\n",
                "\n",
                "# Run the final check\n",
                "check_if_importing_is_successful()\n",
                "\n",
                "if not yelp_graph_ready:\n",
                "    print(\"\\n--- FINAL VERIFICATION FAILED ---\")\n",
                "else:\n",
                "     print(\"\\n--- FINAL VERIFICATION COMPLETE --- \")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.9.7"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}
