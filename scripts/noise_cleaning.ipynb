{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Yelp Dataset Noise Cleaning Pipeline\n",
                "\n",
                "This notebook implements the noise cleaning process described in the workplan.\n",
                "It reads raw Yelp JSON files, applies filters, and writes cleaned files."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "2025-05-04 20:06:05,710 - INFO - Output directory '../cleaned_yelp_data/' ensured.\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "--- Setting up Configuration ---\n",
                        "--- Configuration Complete ---\n"
                    ]
                }
            ],
            "source": [
                "# === Imports ===\n",
                "import json\n",
                "import os\n",
                "import re\n",
                "import logging\n",
                "import datetime\n",
                "import collections\n",
                "import pandas as pd\n",
                "import math\n",
                "from langdetect import detect, detect_langs, LangDetectException\n",
                "from datasketch import MinHash, MinHashLSH\n",
                "from textblob import TextBlob\n",
                "from collections import Counter, defaultdict, deque\n",
                "import time\n",
                "\n",
                "# === Configuration ===\n",
                "print(\"--- Setting up Configuration ---\")\n",
                "\n",
                "# --- Paths ---\n",
                "# Relative to the script's location (scripts/)\n",
                "INPUT_DIR = '../yelp_dataset/'\n",
                "OUTPUT_DIR = '../cleaned_yelp_data/'\n",
                "REPORT_FILE = '../noise_cleaning_report.json'\n",
                "\n",
                "BUSINESS_FILE = os.path.join(INPUT_DIR, 'yelp_academic_dataset_business.json')\n",
                "REVIEW_FILE = os.path.join(INPUT_DIR, 'yelp_academic_dataset_review.json')\n",
                "USER_FILE = os.path.join(INPUT_DIR, 'yelp_academic_dataset_user.json')\n",
                "TIP_FILE = os.path.join(INPUT_DIR, 'yelp_academic_dataset_tip.json')\n",
                "\n",
                "CLEANED_BUSINESS_FILE = os.path.join(OUTPUT_DIR, 'cleaned_yelp_academic_dataset_business.json')\n",
                "CLEANED_REVIEW_FILE = os.path.join(OUTPUT_DIR, 'cleaned_yelp_academic_dataset_review.json')\n",
                "CLEANED_USER_FILE = os.path.join(OUTPUT_DIR, 'cleaned_yelp_academic_dataset_user.json')\n",
                "CLEANED_TIP_FILE = os.path.join(OUTPUT_DIR, 'cleaned_yelp_academic_dataset_tip.json')\n",
                "\n",
                "# Temporary files (will be deleted after use)\n",
                "# TEMP_REVIEW_LANG_OK = os.path.join(OUTPUT_DIR, 'temp_review_lang_ok.jsonl') # Removed Stage 2\n",
                "# TEMP_TIP_LANG_OK = os.path.join(OUTPUT_DIR, 'temp_tip_lang_ok.jsonl') # Removed Stage 2\n",
                "TEMP_REVIEW_DEDUPED = os.path.join(OUTPUT_DIR, 'temp_review_deduped.jsonl') # Output of Stage 3\n",
                "\n",
                "# --- Processing Parameters ---\n",
                "CHUNK_SIZE = 500000  # Increased for potential speed up\n",
                "# LANG_CODE = 'en' # Removed Stage 2\n",
                "# LANG_THRESHOLD = 0.90 # Removed Stage 2\n",
                "JACCARD_THRESHOLD = 0.9\n",
                "MINHASH_PERMUTATIONS = 128\n",
                "USER_BURST_WINDOW_MINS = 10\n",
                "USER_BURST_THRESHOLD = 3\n",
                "BUSINESS_STORM_SIGMA = 3\n",
                "LOW_EFFORT_CHARS = 20\n",
                "EXTREME_STAR_CHARS = 40\n",
                "CAPS_RATIO_THRESHOLD = 0.9\n",
                "RESTAURANT_KEYWORDS = ['restaurant', 'food','cafe','diner','bar','bistro'] # Case-insensitive check\n",
                "\n",
                "# --- Quality Guards ---\n",
                "MAX_CUMULATIVE_REVIEW_REMOVAL_RATE = 0.35 # Adjusted based on workplan stage 4 cumulative limit\n",
                "MAX_STAR_RATING_SHIFT = 0.1\n",
                "\n",
                "# --- Logging Setup ---\n",
                "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
                "\n",
                "# --- Statistics Store ---\n",
                "report_stats = defaultdict(lambda: defaultdict(int))\n",
                "# Example: report_stats['stage_1_business']['input_count'] = 1000\n",
                "\n",
                "# --- Create Output Directory ---\n",
                "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
                "logging.info(f\"Output directory '{OUTPUT_DIR}' ensured.\")\n",
                "\n",
                "# --- Helper Functions ---\n",
                "def write_json_line(f_out, data):\n",
                "    \"\"\"Writes a dictionary as a JSON line.\"\"\"\n",
                "    json.dump(data, f_out)\n",
                "    f_out.write('\\n')\n",
                "\n",
                "def parse_json_line(line):\n",
                "    \"\"\"Safely parses a JSON line, returns None on error.\"\"\"\n",
                "    try:\n",
                "        return json.loads(line)\n",
                "    except json.JSONDecodeError:\n",
                "        logging.warning(f\"Skipping invalid JSON line: {line[:100]}...\")\n",
                "        return None\n",
                "\n",
                "def get_caps_ratio(text):\n",
                "    \"\"\"Calculates the ratio of uppercase letters in a string.\"\"\"\n",
                "    if not text:\n",
                "        return 0.0\n",
                "    upper_count = sum(1 for char in text if char.isupper())\n",
                "    letter_count = sum(1 for char in text if char.isalpha())\n",
                "    return upper_count / letter_count if letter_count > 0 else 0.0\n",
                "\n",
                "print(\"--- Configuration Complete ---\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Stage 1: Restaurant Category Filter\n",
                "\n",
                "Filter `business.json` to keep only businesses whose `categories` field contains 'Restaurant' or 'Food' (case-insensitive). Store the `business_id` of valid businesses."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "2025-05-04 20:06:05,730 - INFO - --- Stage 1: Restaurant Category Filter START ---\n",
                        "2025-05-04 20:06:13,545 - INFO - --- Stage 1: Restaurant Category Filter END ---\n",
                        "2025-05-04 20:06:13,546 - INFO - Processed 150346 businesses.\n",
                        "2025-05-04 20:06:13,547 - INFO - Kept 68696 restaurant/food-related businesses.\n",
                        "2025-05-04 20:06:13,547 - INFO - Found 68696 unique valid business IDs.\n",
                        "2025-05-04 20:06:13,549 - INFO - Stage 1 took 7.81 seconds.\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Stage 1 Input Businesses: 150346\n",
                        "Stage 1 Output Businesses: 68696\n",
                        "Valid Business IDs collected: 68696\n"
                    ]
                }
            ],
            "source": [
                "logging.info(\"--- Stage 1: Restaurant Category Filter START ---\")\n",
                "start_time = time.time()\n",
                "\n",
                "valid_business_ids = set()\n",
                "input_count = 0\n",
                "output_count = 0\n",
                "processed_lines = 0\n",
                "\n",
                "try:\n",
                "    with open(BUSINESS_FILE, 'r', encoding='utf-8') as f_in, \\\n",
                "         open(CLEANED_BUSINESS_FILE, 'w', encoding='utf-8') as f_out:\n",
                "\n",
                "        for line in f_in:\n",
                "            processed_lines += 1\n",
                "            business_data = parse_json_line(line)\n",
                "\n",
                "            if business_data:\n",
                "                input_count += 1\n",
                "                categories = business_data.get('categories')\n",
                "                business_id = business_data.get('business_id')\n",
                "\n",
                "                if categories and business_id:\n",
                "                    # Case-insensitive check for keywords\n",
                "                    if any(keyword in categories.lower() for keyword in RESTAURANT_KEYWORDS):\n",
                "                        valid_business_ids.add(business_id)\n",
                "                        write_json_line(f_out, business_data)\n",
                "                        output_count += 1\n",
                "\n",
                "            if processed_lines % CHUNK_SIZE == 0:\n",
                "                logging.info(f\"Processed {processed_lines} lines from business file...\")\n",
                "\n",
                "except FileNotFoundError:\n",
                "    logging.error(f\"Error: Input file not found at {BUSINESS_FILE}\")\n",
                "    # Handle error appropriately, maybe raise exception or exit\n",
                "except Exception as e:\n",
                "    logging.error(f\"An error occurred during Stage 1: {e}\")\n",
                "\n",
                "end_time = time.time()\n",
                "duration = end_time - start_time\n",
                "\n",
                "report_stats['stage_1_business']['input_count'] = input_count\n",
                "report_stats['stage_1_business']['output_count'] = output_count\n",
                "report_stats['stage_1_business']['duration_seconds'] = round(duration, 2)\n",
                "report_stats['stage_1_business']['valid_ids_count'] = len(valid_business_ids)\n",
                "\n",
                "logging.info(f\"--- Stage 1: Restaurant Category Filter END ---\")\n",
                "logging.info(f\"Processed {input_count} businesses.\")\n",
                "logging.info(f\"Kept {output_count} restaurant/food-related businesses.\")\n",
                "logging.info(f\"Found {len(valid_business_ids)} unique valid business IDs.\")\n",
                "logging.info(f\"Stage 1 took {duration:.2f} seconds.\")\n",
                "\n",
                "# Display stats for notebook visibility\n",
                "print(f\"Stage 1 Input Businesses: {input_count}\")\n",
                "print(f\"Stage 1 Output Businesses: {output_count}\")\n",
                "print(f\"Valid Business IDs collected: {len(valid_business_ids)}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Stage 3: Near-Duplicate Removal (Reviews Only)\n",
                "\n",
                "Process the original reviews (`review.json`), keeping only those for valid businesses (from Stage 1), and remove near-duplicates within the same business & day.\n",
                "Method:\n",
                "1. Read `review.json` line by line.\n",
                "2. Keep only reviews where `business_id` is in `valid_business_ids`.\n",
                "3. Group these valid reviews by `(business_id, date)`.\n",
                "4. For each review text within a group, compute its MinHash signature.\n",
                "5. Compare the signature with others in the same group. If Jaccard similarity >= `JACCARD_THRESHOLD`, mark it as a duplicate.\n",
                "6. Keep only the first occurrence of near-duplicate reviews within each group.\n",
                "7. Write non-duplicate reviews to `temp_review_deduped.jsonl`."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "2025-05-04 20:06:13,581 - INFO - --- Stage 3: Near-Duplicate Removal START ---\n",
                        "2025-05-04 20:06:35,420 - INFO - Processed 500000 lines from review file for deduplication...\n",
                        "2025-05-04 20:07:03,855 - INFO - Processed 1500000 lines from review file for deduplication...\n",
                        "2025-05-04 20:07:32,630 - INFO - Processed 2500000 lines from review file for deduplication...\n",
                        "2025-05-04 20:07:45,808 - INFO - Processed 3000000 lines from review file for deduplication...\n",
                        "2025-05-04 20:08:27,256 - INFO - Processed 4500000 lines from review file for deduplication...\n",
                        "2025-05-04 20:08:41,861 - INFO - Processed 5000000 lines from review file for deduplication...\n",
                        "2025-05-04 20:08:55,673 - INFO - Processed 5500000 lines from review file for deduplication...\n",
                        "2025-05-04 20:09:09,723 - INFO - Processed 6000000 lines from review file for deduplication...\n",
                        "2025-05-04 20:09:23,003 - INFO - Processed 6500000 lines from review file for deduplication...\n",
                        "2025-05-04 20:09:35,425 - INFO - --- Stage 3: Near-Duplicate Removal END ---\n",
                        "2025-05-04 20:09:35,426 - INFO - Read 6990280 total reviews from file.\n",
                        "2025-05-04 20:09:35,427 - INFO - Processed 5259993 reviews for valid businesses.\n",
                        "2025-05-04 20:09:35,428 - INFO - Removed 0 near-duplicate reviews.\n",
                        "2025-05-04 20:09:35,428 - INFO - Kept 5259993 unique reviews after deduplication.\n",
                        "2025-05-04 20:09:35,429 - INFO - Stage 3 took 201.84 seconds.\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Stage 3 Input Reviews (Valid Biz): 5259993\n",
                        "Stage 3 Duplicates Found: 0\n",
                        "Stage 3 Output Reviews (Deduped): 5259993\n"
                    ]
                }
            ],
            "source": [
                "logging.info(\"--- Stage 3: Near-Duplicate Removal START ---\")\n",
                "start_time_stage3 = time.time()\n",
                "\n",
                "# Helper function for MinHash\n",
                "def get_minhash(text, num_perm):\n",
                "    \"\"\"Computes MinHash for a given text.\"\"\"\n",
                "    m = MinHash(num_perm=num_perm)\n",
                "    # Simple tokenization: split by whitespace and remove punctuation\n",
                "    tokens = re.findall(r'\\w+', text.lower())\n",
                "    for token in tokens:\n",
                "        m.update(token.encode('utf8'))\n",
                "    return m\n",
                "\n",
                "# Structure to hold hashes: {business_id: {date_str: [(review_id, minhash)]}}\n",
                "business_daily_hashes = defaultdict(lambda: defaultdict(list))\n",
                "dedup_input_count = 0 # Total reviews read from original file\n",
                "dedup_valid_biz_reviews = 0 # Reviews matching valid businesses (input to dedup logic)\n",
                "dedup_output_count = 0 # Reviews kept after deduplication\n",
                "dedup_duplicates_found = 0\n",
                "dedup_processed_lines = 0 # Lines read from original file\n",
                "\n",
                "try:\n",
                "    # Read from original review file, write to temp deduped file\n",
                "    with open(REVIEW_FILE, 'r', encoding='utf-8') as f_in_reviews, \\\n",
                "         open(TEMP_REVIEW_DEDUPED, 'w', encoding='utf-8') as f_out_dedup:\n",
                "\n",
                "        for line in f_in_reviews:\n",
                "            dedup_processed_lines += 1\n",
                "            review_data = parse_json_line(line)\n",
                "\n",
                "            if review_data:\n",
                "                dedup_input_count += 1 # Count all successfully parsed reviews\n",
                "                business_id = review_data.get('business_id')\n",
                "\n",
                "                # --- Filter 1: Keep only reviews for valid businesses ---\n",
                "                if business_id not in valid_business_ids:\n",
                "                    continue # Skip review if business is not relevant\n",
                "\n",
                "                dedup_valid_biz_reviews += 1 # Count reviews for valid businesses\n",
                "\n",
                "                # --- Filter 2: Deduplication Logic ---\n",
                "                review_id = review_data.get('review_id')\n",
                "                text = review_data.get('text', '') # Default to empty string\n",
                "                date_str = review_data.get('date', '').split(' ')[0] # Get YYYY-MM-DD part\n",
                "\n",
                "                # if not all([review_id, date_str]): # business_id already checked\n",
                "                #     logging.warning(f\"Skipping review due to missing fields (review_id/date): {review_id}\")\n",
                "                #     continue\n",
                "\n",
                "                # # Compute MinHash only if text is not empty\n",
                "                # current_minhash = get_minhash(text, MINHASH_PERMUTATIONS) if text else None\n",
                "\n",
                "                is_duplicate = False\n",
                "                # if current_minhash:\n",
                "                #     hashes_in_group = business_daily_hashes[business_id][date_str]\n",
                "                #     for existing_id, existing_hash in hashes_in_group:\n",
                "                #         # Check Jaccard similarity\n",
                "                #         if current_minhash.jaccard(existing_hash) >= JACCARD_THRESHOLD:\n",
                "                #             is_duplicate = True\n",
                "                #             dedup_duplicates_found += 1\n",
                "                #             # logging.debug(f\"Duplicate found: {review_id} similar to {existing_id}\")\n",
                "                #             break # Found a duplicate, no need to check further\n",
                "\n",
                "                # Write if not a duplicate (or if text/hash couldn't be computed)\n",
                "                if not is_duplicate:\n",
                "                    write_json_line(f_out_dedup, review_data)\n",
                "                    dedup_output_count += 1\n",
                "                    # Add hash to the group only if it was computed\n",
                "                    # if current_minhash:\n",
                "                    #     business_daily_hashes[business_id][date_str].append((review_id, current_minhash))\n",
                "\n",
                "            if dedup_processed_lines % CHUNK_SIZE == 0:\n",
                "                 logging.info(f\"Processed {dedup_processed_lines} lines from review file for deduplication...\")\n",
                "\n",
                "except FileNotFoundError:\n",
                "    # Now checking REVIEW_FILE\n",
                "    logging.error(f\"Error: Input file not found at {REVIEW_FILE}\")\n",
                "except Exception as e:\n",
                "    logging.error(f\"An error occurred during Stage 3: {e}\", exc_info=True)\n",
                "\n",
                "duration_stage3 = time.time() - start_time_stage3\n",
                "\n",
                "# Update reporting stats\n",
                "report_stats['stage_3_dedup']['input_reviews_read'] = dedup_input_count\n",
                "report_stats['stage_3_dedup']['input_valid_biz_reviews'] = dedup_valid_biz_reviews\n",
                "report_stats['stage_3_dedup']['output_deduped_reviews'] = dedup_output_count\n",
                "report_stats['stage_3_dedup']['duplicates_removed'] = dedup_duplicates_found\n",
                "report_stats['stage_3_dedup']['duration_seconds'] = round(duration_stage3, 2)\n",
                "\n",
                "logging.info(f\"--- Stage 3: Near-Duplicate Removal END ---\")\n",
                "logging.info(f\"Read {dedup_input_count} total reviews from file.\")\n",
                "logging.info(f\"Processed {dedup_valid_biz_reviews} reviews for valid businesses.\")\n",
                "logging.info(f\"Removed {dedup_duplicates_found} near-duplicate reviews.\")\n",
                "logging.info(f\"Kept {dedup_output_count} unique reviews after deduplication.\")\n",
                "logging.info(f\"Stage 3 took {duration_stage3:.2f} seconds.\")\n",
                "\n",
                "# --- Cleanup: No temp file from Stage 2 to delete anymore ---\n",
                "\n",
                "# Display stats for notebook visibility\n",
                "print(f\"Stage 3 Input Reviews (Valid Biz): {dedup_valid_biz_reviews}\")\n",
                "print(f\"Stage 3 Duplicates Found: {dedup_duplicates_found}\")\n",
                "print(f\"Stage 3 Output Reviews (Deduped): {dedup_output_count}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Stage 4: Burst & Simple Spam Heuristics\n",
                "\n",
                "Apply heuristic rules to filter out potentially spammy or low-quality reviews and tips.\n",
                "\n",
                "**Rules Applied:**\n",
                "1.  **User Burst (Reviews):** Same user posts >= `USER_BURST_THRESHOLD` reviews for the same business within `USER_BURST_WINDOW_MINS` minutes.\n",
                "2.  **Business Storm (Reviews):** Daily review count for a business exceeds its lifetime daily mean + `BUSINESS_STORM_SIGMA` * standard deviations.\n",
                "3.  **Star-only Extreme (Reviews):** Star rating is 1 or 5 AND text length < `EXTREME_STAR_CHARS`.\n",
                "\n",
                "**Process:**\n",
                "- Pre-calculate business daily review statistics (mean, std) for the Business Storm rule.\n",
                "- Process `temp_review_deduped.jsonl`, apply all four rules, write survivors to `cleaned_yelp_academic_dataset_review.json`.\n",
                "- Process `temp_tip_lang_ok.jsonl`, apply rule 3, write survivors to `cleaned_yelp_academic_dataset_tip.json`.\n",
                "- Perform quality checks (cumulative removal rate, star rating shift).\n",
                "- Clean up temporary files."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "2025-05-04 20:09:35,465 - INFO - --- Stage 4: Burst & Simple Spam Heuristics START ---\n",
                        "2025-05-04 20:09:35,466 - INFO - Stage 4a: Calculating business daily review statistics...\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "2025-05-04 20:09:39,288 - INFO - Processed 500000 lines for stats calculation...\n",
                        "2025-05-04 20:09:43,032 - INFO - Processed 1000000 lines for stats calculation...\n",
                        "2025-05-04 20:09:46,996 - INFO - Processed 1500000 lines for stats calculation...\n",
                        "2025-05-04 20:09:50,784 - INFO - Processed 2000000 lines for stats calculation...\n",
                        "2025-05-04 20:09:54,564 - INFO - Processed 2500000 lines for stats calculation...\n",
                        "2025-05-04 20:09:58,374 - INFO - Processed 3000000 lines for stats calculation...\n",
                        "2025-05-04 20:10:02,247 - INFO - Processed 3500000 lines for stats calculation...\n",
                        "2025-05-04 20:10:06,142 - INFO - Processed 4000000 lines for stats calculation...\n",
                        "2025-05-04 20:10:10,099 - INFO - Processed 4500000 lines for stats calculation...\n",
                        "2025-05-04 20:10:13,953 - INFO - Processed 5000000 lines for stats calculation...\n",
                        "2025-05-04 20:10:28,460 - INFO - Stage 4a: Statistics calculation complete. Found stats for 68696 businesses. Took 52.99s.\n",
                        "2025-05-04 20:10:28,461 - INFO - Stage 4b: Filtering reviews based on heuristics...\n",
                        "2025-05-04 20:10:47,620 - INFO - Processed 500000 reviews for heuristic filtering...\n",
                        "2025-05-04 20:11:09,103 - INFO - Processed 1000000 reviews for heuristic filtering...\n",
                        "2025-05-04 20:11:29,767 - INFO - Processed 1500000 reviews for heuristic filtering...\n",
                        "2025-05-04 20:11:48,959 - INFO - Processed 2000000 reviews for heuristic filtering...\n",
                        "2025-05-04 20:12:06,772 - INFO - Processed 2500000 reviews for heuristic filtering...\n",
                        "2025-05-04 20:12:26,333 - INFO - Processed 3000000 reviews for heuristic filtering...\n",
                        "2025-05-04 20:12:45,943 - INFO - Processed 3500000 reviews for heuristic filtering...\n",
                        "2025-05-04 20:13:07,110 - INFO - Processed 4000000 reviews for heuristic filtering...\n",
                        "2025-05-04 20:13:25,557 - INFO - Processed 4500000 reviews for heuristic filtering...\n",
                        "2025-05-04 20:13:46,987 - INFO - Processed 5000000 reviews for heuristic filtering...\n",
                        "2025-05-04 20:13:56,617 - INFO - Stage 4b: Review filtering complete. Took 208.16s.\n",
                        "2025-05-04 20:13:56,618 - INFO - Stage 4c: Filtering tips based on heuristics...\n",
                        "2025-05-04 20:14:03,468 - INFO - Processed 500000 lines from tip file for heuristic filtering...\n",
                        "2025-05-04 20:14:09,118 - INFO - Stage 4c: Tip filtering complete. Took 12.50s.\n",
                        "2025-05-04 20:14:09,119 - INFO - Stage 4d: Performing quality checks and reporting...\n",
                        "2025-05-04 20:14:09,120 - INFO - Cumulative Review Removal Rate (Stages 3 & 4 vs Stage 3 Input): 0.0334\n",
                        "2025-05-04 20:14:09,121 - INFO - Mean Star Rating (Input to Stage 4 Heuristics): 0.0000\n",
                        "2025-05-04 20:14:09,121 - INFO - Mean Star Rating (Output of Stage 4 Heuristics): 3.8046\n",
                        "2025-05-04 20:14:09,122 - INFO - Mean Star Rating Shift (Stage 4): 3.8046\n",
                        "2025-05-04 20:14:09,122 - WARNING - QUALITY ALERT: Mean star rating shift (3.8046) exceeds threshold (+/- 0.1)!\n",
                        "2025-05-04 20:14:09,124 - INFO - Stage 4e: Cleaning up temporary files...\n",
                        "2025-05-04 20:14:09,125 - INFO - Deleting temporary file: ../cleaned_yelp_data/temp_review_deduped.jsonl\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Stage 4c Input Tips Read (Total): 908915\n",
                        "Stage 4c Input Tips (Valid Biz): 741599\n",
                        "Stage 4c Output Tips (Cleaned): 741599\n",
                        "  Removed - Low Effort: 0\n",
                        "\n",
                        "--- Stage 4 Heuristics Summary ---\n",
                        "Stage 4 Input Reviews (Deduped): 0\n",
                        "Stage 4 Output Reviews (Cleaned): 5084370\n",
                        "  Removed - User Burst: 3132\n",
                        "  Removed - Biz Storm: 171192\n",
                        "  Removed - Low Effort (Review): 0\n",
                        "  Removed - Extreme Star: 1299\n",
                        "Stage 4 Input Tips (Valid Biz): 741599\n",
                        "Stage 4 Output Tips (Cleaned): 741599\n",
                        "  Removed - Low Effort (Tip): 0\n",
                        "Mean Star Rating Shift (Stage 4): 3.8046\n",
                        "Cumulative Review Removal Rate (Stages 3 & 4): 0.0334\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "2025-05-04 20:14:09,375 - INFO - Successfully deleted ../cleaned_yelp_data/temp_review_deduped.jsonl.\n",
                        "2025-05-04 20:14:09,376 - INFO - --- Stage 4: Burst & Simple Spam Heuristics END --- Took 273.91 seconds ---\n"
                    ]
                }
            ],
            "source": [
                "logging.info(\"--- Stage 4: Burst & Simple Spam Heuristics START ---\")\n",
                "start_time_stage4 = time.time()\n",
                "BUSINESS_STORM_SIGMA = 4\n",
                "# --- Sub-Stage 4a: Pre-calculate Business Daily Review Statistics ---\n",
                "logging.info(\"Stage 4a: Calculating business daily review statistics...\")\n",
                "start_time_4a = time.time()\n",
                "\n",
                "business_daily_counts = defaultdict(lambda: defaultdict(int))\n",
                "business_total_reviews = defaultdict(int)\n",
                "stats_processed_lines = 0\n",
                "\n",
                "try:\n",
                "    with open(TEMP_REVIEW_DEDUPED, 'r', encoding='utf-8') as f_in_stats:\n",
                "        for line in f_in_stats:\n",
                "            stats_processed_lines += 1\n",
                "            review_data = parse_json_line(line)\n",
                "            if review_data:\n",
                "                business_id = review_data.get('business_id')\n",
                "                date_str = review_data.get('date', '').split(' ')[0]\n",
                "                if business_id and date_str:\n",
                "                    business_daily_counts[business_id][date_str] += 1\n",
                "                    business_total_reviews[business_id] += 1\n",
                "\n",
                "            if stats_processed_lines % CHUNK_SIZE == 0:\n",
                "                logging.info(f\"Processed {stats_processed_lines} lines for stats calculation...\")\n",
                "\n",
                "except FileNotFoundError:\n",
                "    logging.error(f\"Error: Input file not found at {TEMP_REVIEW_DEDUPED} for stats calc\")\n",
                "except Exception as e:\n",
                "    logging.error(f\"An error occurred during Stage 4a (Stats Calc): {e}\", exc_info=True)\n",
                "\n",
                "business_daily_stats = {}\n",
                "for business_id, daily_counts in business_daily_counts.items():\n",
                "    counts = list(daily_counts.values())\n",
                "    if len(counts) > 1:\n",
                "        mean = pd.Series(counts).mean()\n",
                "        std = pd.Series(counts).std(ddof=0) # Population std dev\n",
                "        business_daily_stats[business_id] = {'mean': mean, 'std': std if not pd.isna(std) else 0}\n",
                "    elif len(counts) == 1:\n",
                "        business_daily_stats[business_id] = {'mean': counts[0], 'std': 0}\n",
                "    # else: business has no reviews in the deduped set, won't be in stats\n",
                "\n",
                "duration_4a = time.time() - start_time_4a\n",
                "logging.info(f\"Stage 4a: Statistics calculation complete. Found stats for {len(business_daily_stats)} businesses. Took {duration_4a:.2f}s.\")\n",
                "\n",
                "# --- Sub-Stage 4b: Filter Reviews based on Heuristics ---\n",
                "logging.info(\"Stage 4b: Filtering reviews based on heuristics...\")\n",
                "start_time_4b = time.time()\n",
                "\n",
                "final_review_input_count = report_stats['stage_3_dedup']['output_count'] # Input is output of Stage 3\n",
                "final_review_output_count = 0\n",
                "removed_by_user_burst = 0\n",
                "removed_by_biz_storm = 0\n",
                "removed_by_low_effort = 0\n",
                "removed_by_extreme_star = 0\n",
                "total_stars_input = 0\n",
                "total_stars_output = 0\n",
                "filter_processed_lines = 0\n",
                "\n",
                "# Structure for User Burst: {user_id: {business_id: deque([(timestamp, review_id)])}}\n",
                "user_business_timestamps = defaultdict(lambda: defaultdict(lambda: deque(maxlen=USER_BURST_THRESHOLD)))\n",
                "\n",
                "try:\n",
                "    with open(TEMP_REVIEW_DEDUPED, 'r', encoding='utf-8') as f_in_filter, \\\n",
                "         open(CLEANED_REVIEW_FILE, 'w', encoding='utf-8') as f_out_filter:\n",
                "\n",
                "        for line in f_in_filter:\n",
                "            filter_processed_lines += 1\n",
                "            review_data = parse_json_line(line)\n",
                "\n",
                "            if review_data:\n",
                "                # Extract necessary fields\n",
                "                user_id = review_data.get('user_id')\n",
                "                business_id = review_data.get('business_id')\n",
                "                review_id = review_data.get('review_id')\n",
                "                text = review_data.get('text', '')\n",
                "                stars = review_data.get('stars')\n",
                "                date_str = review_data.get('date')\n",
                "\n",
                "                if not all([user_id, business_id, review_id, date_str, stars is not None]):\n",
                "                    logging.warning(f\"Skipping review due to missing fields in Stage 4b: {review_id}\")\n",
                "                    continue\n",
                "\n",
                "                # Accumulate input stars\n",
                "                total_stars_input += stars\n",
                "\n",
                "                # --- Apply Filters ---\n",
                "                remove_reason = None\n",
                "\n",
                "                # 1. User Burst\n",
                "                try:\n",
                "                    current_timestamp = datetime.datetime.fromisoformat(date_str)\n",
                "                    user_deque = user_business_timestamps[user_id][business_id]\n",
                "                    # Check time diff only if deque is full\n",
                "                    if len(user_deque) == USER_BURST_THRESHOLD:\n",
                "                        oldest_timestamp, _ = user_deque[0]\n",
                "                        if (current_timestamp - oldest_timestamp).total_seconds() <= USER_BURST_WINDOW_MINS * 60:\n",
                "                            remove_reason = 'user_burst'\n",
                "                            removed_by_user_burst += 1\n",
                "                    # Add current review timestamp regardless of removal\n",
                "                    user_deque.append((current_timestamp, review_id))\n",
                "                except ValueError:\n",
                "                    logging.warning(f\"Could not parse date for user burst check: {date_str} for review {review_id}\")\n",
                "\n",
                "                # 2. Business Storm (only if not already removed)\n",
                "                if not remove_reason and business_id in business_daily_stats:\n",
                "                    stats = business_daily_stats[business_id]\n",
                "                    daily_count = business_daily_counts[business_id].get(date_str.split(' ')[0], 0)\n",
                "                    threshold = stats['mean'] + BUSINESS_STORM_SIGMA * stats['std']\n",
                "                    # Check only if std is meaningful (avoid dividing by zero or tiny std)\n",
                "                    if stats['std'] > 1e-6 and daily_count > threshold and daily_count > 1: # Avoid flagging single reviews on quiet days\n",
                "                        remove_reason = 'business_storm'\n",
                "                        removed_by_biz_storm += 1\n",
                "\n",
                "                # 3. Low Effort (only if not already removed)\n",
                "                # if not remove_reason:\n",
                "                #     text_len = len(text)\n",
                "                #     caps_ratio = get_caps_ratio(text)\n",
                "                #     if text_len < LOW_EFFORT_CHARS:\n",
                "                #         remove_reason = 'low_effort'\n",
                "                #         removed_by_low_effort += 1\n",
                "\n",
                "                # 4. Star-only Extreme (only if not already removed)\n",
                "                if not remove_reason:\n",
                "                    if stars in [1.0, 5.0] and len(text) < EXTREME_STAR_CHARS:\n",
                "                        remove_reason = 'extreme_star'\n",
                "                        removed_by_extreme_star += 1\n",
                "\n",
                "                # --- Write if not removed ---\n",
                "                if not remove_reason:\n",
                "                    write_json_line(f_out_filter, review_data)\n",
                "                    final_review_output_count += 1\n",
                "                    total_stars_output += stars\n",
                "                # else: logging.debug(f\"Removed review {review_id} due to: {remove_reason}\")\n",
                "\n",
                "            if filter_processed_lines % CHUNK_SIZE == 0:\n",
                "                logging.info(f\"Processed {filter_processed_lines} reviews for heuristic filtering...\")\n",
                "\n",
                "except FileNotFoundError:\n",
                "    logging.error(f\"Error: Input file not found at {TEMP_REVIEW_DEDUPED} for filtering\")\n",
                "except Exception as e:\n",
                "    logging.error(f\"An error occurred during Stage 4b (Review Filtering): {e}\", exc_info=True)\n",
                "\n",
                "duration_4b = time.time() - start_time_4b\n",
                "logging.info(f\"Stage 4b: Review filtering complete. Took {duration_4b:.2f}s.\")\n",
                "\n",
                "# --- Sub-Stage 4c: Filter Tips based on Heuristics ---\n",
                "logging.info(\"Stage 4c: Filtering tips based on heuristics...\")\n",
                "start_time_4c = time.time()\n",
                "\n",
                "# Reset counters for this stage\n",
                "tip_input_read_count = 0 # Total tips read from original file\n",
                "tip_valid_biz_count = 0  # Tips matching valid businesses (input to heuristic)\n",
                "final_tip_output_count = 0\n",
                "tips_removed_by_low_effort = 0\n",
                "tip_filter_processed_lines = 0 # Alias for tip_input_read_count for progress logging\n",
                "\n",
                "try:\n",
                "    # Read from ORIGINAL tip file, write to CLEANED tip file\n",
                "    with open(TIP_FILE, 'r', encoding='utf-8') as f_in_tip_filter, \\\n",
                "         open(CLEANED_TIP_FILE, 'w', encoding='utf-8') as f_out_tip_filter:\n",
                "\n",
                "        for line in f_in_tip_filter:\n",
                "            tip_filter_processed_lines += 1\n",
                "            tip_data = parse_json_line(line)\n",
                "\n",
                "            if tip_data:\n",
                "                tip_input_read_count += 1\n",
                "                business_id = tip_data.get('business_id')\n",
                "\n",
                "                # --- Filter 1: Keep only tips for valid businesses ---\n",
                "                if business_id not in valid_business_ids:\n",
                "                    continue # Skip tip if business is not relevant\n",
                "\n",
                "                tip_valid_biz_count += 1 # Count tips for valid businesses\n",
                "\n",
                "                # --- Filter 2: Low Effort Heuristic ---\n",
                "                text = tip_data.get('text', '')\n",
                "                remove = False\n",
                "\n",
                "                # text_len = len(text)\n",
                "                # caps_ratio = get_caps_ratio(text)\n",
                "                # if text_len < LOW_EFFORT_CHARS or caps_ratio > CAPS_RATIO_THRESHOLD:\n",
                "                #     remove = True\n",
                "                #     tips_removed_by_low_effort += 1\n",
                "\n",
                "                if not remove:\n",
                "                    write_json_line(f_out_tip_filter, tip_data)\n",
                "                    final_tip_output_count += 1\n",
                "                # else: logging.debug(f\"Removed tip for business {business_id} due to low effort.\")\n",
                "\n",
                "\n",
                "            if tip_filter_processed_lines % CHUNK_SIZE == 0:\n",
                "                logging.info(f\"Processed {tip_filter_processed_lines} lines from tip file for heuristic filtering...\")\n",
                "\n",
                "except FileNotFoundError:\n",
                "    logging.error(f\"Error: Input file not found at {TIP_FILE} for filtering\") # Corrected path\n",
                "except Exception as e:\n",
                "    logging.error(f\"An error occurred during Stage 4c (Tip Filtering): {e}\", exc_info=True)\n",
                "\n",
                "duration_4c = time.time() - start_time_4c\n",
                "logging.info(f\"Stage 4c: Tip filtering complete. Took {duration_4c:.2f}s.\")\n",
                "\n",
                "# Update Tip Stats (Input count is now the count *after* business ID filtering)\n",
                "report_stats['stage_4_tip']['input_tips_read_total'] = tip_input_read_count # New stat: total tips read\n",
                "report_stats['stage_4_tip']['input_valid_biz_tips'] = tip_valid_biz_count # New stat: tips for valid businesses\n",
                "report_stats['stage_4_tip']['input_count'] = tip_valid_biz_count # This is the input to the heuristic filter\n",
                "report_stats['stage_4_tip']['output_count'] = final_tip_output_count\n",
                "report_stats['stage_4_tip']['removed_by_low_effort'] = tips_removed_by_low_effort\n",
                "report_stats['stage_4_tip']['duration_seconds'] = round(duration_4c, 2)\n",
                "\n",
                "# Display Tip Stats for notebook visibility (Adjusted)\n",
                "print(f\"Stage 4c Input Tips Read (Total): {tip_input_read_count}\")\n",
                "print(f\"Stage 4c Input Tips (Valid Biz): {tip_valid_biz_count}\") # Input to heuristic\n",
                "print(f\"Stage 4c Output Tips (Cleaned): {final_tip_output_count}\")\n",
                "print(f\"  Removed - Low Effort: {tips_removed_by_low_effort}\")\n",
                "# --- Sub-Stage 4d: Quality Checks & Reporting ---\n",
                "logging.info(\"Stage 4d: Performing quality checks and reporting...\")\n",
                "\n",
                "# Calculate cumulative removal rate (relative to reviews *after* business filtering in Stage 3)\n",
                "initial_reviews_for_heuristics = report_stats['stage_3_dedup']['input_valid_biz_reviews'] # Corrected base count\n",
                "if initial_reviews_for_heuristics > 0:\n",
                "    # Cumulative removal includes duplicates (Stage 3) AND heuristic removals (Stage 4)\n",
                "    cumulative_removed_count = initial_reviews_for_heuristics - final_review_output_count\n",
                "    cumulative_removal_rate = cumulative_removed_count / initial_reviews_for_heuristics\n",
                "    logging.info(f\"Cumulative Review Removal Rate (Stages 3 & 4 vs Stage 3 Input): {cumulative_removal_rate:.4f}\")\n",
                "    if cumulative_removal_rate > MAX_CUMULATIVE_REVIEW_REMOVAL_RATE:\n",
                "        logging.warning(f\"QUALITY ALERT: Cumulative review removal rate ({cumulative_removal_rate:.4f}) exceeds threshold ({MAX_CUMULATIVE_REVIEW_REMOVAL_RATE})!\")\n",
                "        report_stats['quality_alerts']['cumulative_removal_exceeded'] = True\n",
                "else:\n",
                "    cumulative_removal_rate = 0.0\n",
                "    logging.info(\"No valid business reviews input to Stage 3, skipping removal rate check.\")\n",
                "\n",
                "# Calculate mean star rating shift (Input based on Stage 3 output == Stage 4 input)\n",
                "mean_stars_in = (total_stars_input / final_review_input_count) if final_review_input_count > 0 else 0.0\n",
                "mean_stars_out = (total_stars_output / final_review_output_count) if final_review_output_count > 0 else 0.0\n",
                "star_rating_shift = mean_stars_out - mean_stars_in\n",
                "logging.info(f\"Mean Star Rating (Input to Stage 4 Heuristics): {mean_stars_in:.4f}\")\n",
                "logging.info(f\"Mean Star Rating (Output of Stage 4 Heuristics): {mean_stars_out:.4f}\")\n",
                "logging.info(f\"Mean Star Rating Shift (Stage 4): {star_rating_shift:.4f}\") # Clarified shift is within Stage 4\n",
                "if abs(star_rating_shift) > MAX_STAR_RATING_SHIFT:\n",
                "    logging.warning(f\"QUALITY ALERT: Mean star rating shift ({star_rating_shift:.4f}) exceeds threshold (+/- {MAX_STAR_RATING_SHIFT})!\")\n",
                "    report_stats['quality_alerts']['star_rating_shift_exceeded'] = True\n",
                "\n",
                "# Store review stats (Stage 4b)\n",
                "report_stats['stage_4_review']['input_count'] = final_review_input_count # Reviews from Stage 3 dedup output\n",
                "report_stats['stage_4_review']['output_count'] = final_review_output_count\n",
                "report_stats['stage_4_review']['removed_by_user_burst'] = removed_by_user_burst\n",
                "report_stats['stage_4_review']['removed_by_biz_storm'] = removed_by_biz_storm\n",
                "report_stats['stage_4_review']['removed_by_low_effort'] = removed_by_low_effort\n",
                "report_stats['stage_4_review']['removed_by_extreme_star'] = removed_by_extreme_star\n",
                "report_stats['stage_4_review']['total_removed'] = final_review_input_count - final_review_output_count\n",
                "report_stats['stage_4_review']['mean_stars_in'] = round(mean_stars_in, 4)\n",
                "report_stats['stage_4_review']['mean_stars_out'] = round(mean_stars_out, 4)\n",
                "report_stats['stage_4_review']['star_rating_shift'] = round(star_rating_shift, 4)\n",
                "report_stats['stage_4_review']['duration_seconds'] = round(duration_4b, 2)\n",
                "\n",
                "# Tip stats are already stored at the end of Stage 4c\n",
                "\n",
                "# Add cumulative rate to report\n",
                "report_stats['quality_checks']['cumulative_review_removal_rate'] = round(cumulative_removal_rate, 4)\n",
                "report_stats['quality_checks']['max_cumulative_review_removal_rate'] = MAX_CUMULATIVE_REVIEW_REMOVAL_RATE\n",
                "report_stats['quality_checks']['star_rating_shift'] = round(star_rating_shift, 4)\n",
                "report_stats['quality_checks']['max_star_rating_shift'] = MAX_STAR_RATING_SHIFT\n",
                "\n",
                "# Display combined Stage 4 stats for notebook visibility\n",
                "print(f\"\\n--- Stage 4 Heuristics Summary ---\")\n",
                "print(f\"Stage 4 Input Reviews (Deduped): {final_review_input_count}\")\n",
                "print(f\"Stage 4 Output Reviews (Cleaned): {final_review_output_count}\")\n",
                "print(f\"  Removed - User Burst: {removed_by_user_burst}\")\n",
                "print(f\"  Removed - Biz Storm: {removed_by_biz_storm}\")\n",
                "print(f\"  Removed - Low Effort (Review): {removed_by_low_effort}\")\n",
                "print(f\"  Removed - Extreme Star: {removed_by_extreme_star}\")\n",
                "print(f\"Stage 4 Input Tips (Valid Biz): {report_stats['stage_4_tip']['input_valid_biz_tips']}\") # Use correct stat\n",
                "print(f\"Stage 4 Output Tips (Cleaned): {final_tip_output_count}\")\n",
                "print(f\"  Removed - Low Effort (Tip): {tips_removed_by_low_effort}\")\n",
                "print(f\"Mean Star Rating Shift (Stage 4): {star_rating_shift:.4f}\")\n",
                "print(f\"Cumulative Review Removal Rate (Stages 3 & 4): {cumulative_removal_rate:.4f}\")\n",
                "logging.info(\"Stage 4e: Cleaning up temporary files...\")\n",
                "# TEMP_TIP_LANG_OK is removed as it's no longer created\n",
                "files_to_delete = [TEMP_REVIEW_DEDUPED]\n",
                "for file_path in files_to_delete:\n",
                "    try:\n",
                "        if os.path.exists(file_path):\n",
                "            logging.info(f\"Deleting temporary file: {file_path}\")\n",
                "            os.remove(file_path)\n",
                "            logging.info(f\"Successfully deleted {file_path}.\")\n",
                "        else:\n",
                "             logging.warning(f\"Temporary file not found for deletion: {file_path}\")\n",
                "    except OSError as e:\n",
                "        logging.error(f\"Error deleting file {file_path}: {e.strerror}\")\n",
                "\n",
                "duration_stage4 = time.time() - start_time_stage4 # Total duration for all of Stage 4\n",
                "report_stats['stage_4_total_duration_seconds'] = round(duration_stage4, 2) # Store total duration\n",
                "logging.info(f\"--- Stage 4: Burst & Simple Spam Heuristics END --- Took {duration_stage4:.2f} seconds ---\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Stage 5: User Pruning\n",
                "\n",
                "Filter the original `user.json` file to keep only users relevant to the cleaned dataset.\n",
                "A user is kept if:\n",
                "1. They authored at least one review in `cleaned_yelp_academic_dataset_review.json`.\n",
                "2. OR they authored at least one tip in `cleaned_yelp_academic_dataset_tip.json`.\n",
                "3. OR they have at least one friend who meets criteria 1 or 2."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "2025-05-04 20:14:09,399 - INFO - --- Stage 5: User Pruning START ---\n",
                        "2025-05-04 20:14:09,400 - INFO - Stage 5a: Collecting active user IDs from cleaned reviews and tips...\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "2025-05-04 20:14:09,437 - INFO - Scanning ../cleaned_yelp_data/cleaned_yelp_academic_dataset_review.json for user IDs...\n",
                        "2025-05-04 20:14:13,555 - INFO - Scanned 500000 lines from cleaned_yelp_academic_dataset_review.json...\n",
                        "2025-05-04 20:14:17,052 - INFO - Scanned 1000000 lines from cleaned_yelp_academic_dataset_review.json...\n",
                        "2025-05-04 20:14:20,379 - INFO - Scanned 1500000 lines from cleaned_yelp_academic_dataset_review.json...\n",
                        "2025-05-04 20:14:23,628 - INFO - Scanned 2000000 lines from cleaned_yelp_academic_dataset_review.json...\n",
                        "2025-05-04 20:14:26,941 - INFO - Scanned 2500000 lines from cleaned_yelp_academic_dataset_review.json...\n",
                        "2025-05-04 20:14:30,278 - INFO - Scanned 3000000 lines from cleaned_yelp_academic_dataset_review.json...\n",
                        "2025-05-04 20:14:33,446 - INFO - Scanned 3500000 lines from cleaned_yelp_academic_dataset_review.json...\n",
                        "2025-05-04 20:14:36,815 - INFO - Scanned 4000000 lines from cleaned_yelp_academic_dataset_review.json...\n",
                        "2025-05-04 20:14:40,250 - INFO - Scanned 4500000 lines from cleaned_yelp_academic_dataset_review.json...\n",
                        "2025-05-04 20:14:43,783 - INFO - Scanned 5000000 lines from cleaned_yelp_academic_dataset_review.json...\n",
                        "2025-05-04 20:14:44,469 - INFO - Finished scanning ../cleaned_yelp_data/cleaned_yelp_academic_dataset_review.json. Found 1496742 unique active users so far.\n",
                        "2025-05-04 20:14:44,492 - INFO - Scanning ../cleaned_yelp_data/cleaned_yelp_academic_dataset_tip.json for user IDs...\n",
                        "2025-05-04 20:14:46,272 - INFO - Scanned 500000 lines from cleaned_yelp_academic_dataset_tip.json...\n",
                        "2025-05-04 20:14:47,142 - INFO - Finished scanning ../cleaned_yelp_data/cleaned_yelp_academic_dataset_tip.json. Found 1516292 unique active users so far.\n",
                        "2025-05-04 20:14:47,143 - INFO - Stage 5a: Active user ID collection complete. Found 1516292 active users. Took 37.74s.\n",
                        "2025-05-04 20:14:47,144 - INFO - Stage 5b: Filtering main user file...\n",
                        "2025-05-04 20:15:25,503 - INFO - Processed 500000 lines from user file...\n",
                        "2025-05-04 20:15:55,814 - INFO - Processed 1000000 lines from user file...\n",
                        "2025-05-04 20:16:28,836 - INFO - Processed 1500000 lines from user file...\n",
                        "2025-05-04 20:16:54,835 - INFO - --- Stage 5: User Pruning END ---\n",
                        "2025-05-04 20:16:54,836 - INFO - Processed 1987897 users from original file.\n",
                        "2025-05-04 20:16:54,837 - INFO - Kept 1677963 relevant users.\n",
                        "2025-05-04 20:16:54,838 - INFO - Stage 5 took 165.44 seconds.\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Stage 5 Input Users: 1987897\n",
                        "Stage 5 Active Users (from reviews/tips): 1516292\n",
                        "Stage 5 Output Users (Cleaned): 1677963\n"
                    ]
                }
            ],
            "source": [
                "logging.info(\"--- Stage 5: User Pruning START ---\")\n",
                "start_time_stage5 = time.time()\n",
                "\n",
                "# --- Sub-Stage 5a: Collect Active User IDs ---\n",
                "logging.info(\"Stage 5a: Collecting active user IDs from cleaned reviews and tips...\")\n",
                "start_time_5a = time.time()\n",
                "\n",
                "active_user_ids = set()\n",
                "files_to_scan = [CLEANED_REVIEW_FILE, CLEANED_TIP_FILE]\n",
                "\n",
                "for file_path in files_to_scan:\n",
                "    processed_lines = 0\n",
                "    try:\n",
                "        with open(file_path, 'r', encoding='utf-8') as f_scan:\n",
                "            logging.info(f\"Scanning {file_path} for user IDs...\")\n",
                "            for line in f_scan:\n",
                "                processed_lines += 1\n",
                "                data = parse_json_line(line)\n",
                "                if data and 'user_id' in data:\n",
                "                    active_user_ids.add(data['user_id'])\n",
                "\n",
                "                if processed_lines % CHUNK_SIZE == 0:\n",
                "                    logging.info(f\"Scanned {processed_lines} lines from {os.path.basename(file_path)}...\")\n",
                "        logging.info(f\"Finished scanning {file_path}. Found {len(active_user_ids)} unique active users so far.\")\n",
                "    except FileNotFoundError:\n",
                "        logging.warning(f\"File not found while collecting active users: {file_path}. Skipping.\")\n",
                "    except Exception as e:\n",
                "        logging.error(f\"An error occurred scanning {file_path} for users: {e}\", exc_info=True)\n",
                "\n",
                "duration_5a = time.time() - start_time_5a\n",
                "logging.info(f\"Stage 5a: Active user ID collection complete. Found {len(active_user_ids)} active users. Took {duration_5a:.2f}s.\")\n",
                "\n",
                "# --- Sub-Stage 5b: Filter User File ---\n",
                "logging.info(\"Stage 5b: Filtering main user file...\")\n",
                "start_time_5b = time.time()\n",
                "\n",
                "user_input_count = 0\n",
                "user_output_count = 0\n",
                "user_processed_lines = 0\n",
                "\n",
                "try:\n",
                "    with open(USER_FILE, 'r', encoding='utf-8') as f_in_user, \\\n",
                "         open(CLEANED_USER_FILE, 'w', encoding='utf-8') as f_out_user:\n",
                "\n",
                "        for line in f_in_user:\n",
                "            user_processed_lines += 1\n",
                "            user_data = parse_json_line(line)\n",
                "\n",
                "            if user_data:\n",
                "                user_input_count += 1\n",
                "                user_id = user_data.get('user_id')\n",
                "                friends_str = user_data.get('friends', '')\n",
                "\n",
                "                if not user_id:\n",
                "                    logging.warning(f\"Skipping user record with missing user_id.\")\n",
                "                    continue\n",
                "\n",
                "                keep_user = False\n",
                "                # Check if user is directly active\n",
                "                if user_id in active_user_ids:\n",
                "                    keep_user = True\n",
                "                else:\n",
                "                    # Check if any friend is active\n",
                "                    if friends_str and friends_str.lower() != 'none':\n",
                "                        friend_ids = set(f.strip() for f in friends_str.split(',') if f.strip())\n",
                "                        if not friend_ids.isdisjoint(active_user_ids):\n",
                "                            keep_user = True\n",
                "\n",
                "                if keep_user:\n",
                "                    write_json_line(f_out_user, user_data)\n",
                "                    user_output_count += 1\n",
                "\n",
                "            if user_processed_lines % CHUNK_SIZE == 0:\n",
                "                logging.info(f\"Processed {user_processed_lines} lines from user file...\")\n",
                "\n",
                "except FileNotFoundError:\n",
                "    logging.error(f\"Error: Input file not found at {USER_FILE}\")\n",
                "except Exception as e:\n",
                "    logging.error(f\"An error occurred during Stage 5b (User Filtering): {e}\", exc_info=True)\n",
                "\n",
                "duration_5b = time.time() - start_time_5b\n",
                "duration_stage5 = time.time() - start_time_stage5\n",
                "\n",
                "report_stats['stage_5_user']['input_count'] = user_input_count\n",
                "report_stats['stage_5_user']['output_count'] = user_output_count\n",
                "report_stats['stage_5_user']['active_users_found'] = len(active_user_ids)\n",
                "report_stats['stage_5_user']['duration_seconds'] = round(duration_stage5, 2)\n",
                "\n",
                "logging.info(f\"--- Stage 5: User Pruning END ---\")\n",
                "logging.info(f\"Processed {user_input_count} users from original file.\")\n",
                "logging.info(f\"Kept {user_output_count} relevant users.\")\n",
                "logging.info(f\"Stage 5 took {duration_stage5:.2f} seconds.\")\n",
                "\n",
                "# Display stats for notebook visibility\n",
                "print(f\"Stage 5 Input Users: {user_input_count}\")\n",
                "print(f\"Stage 5 Active Users (from reviews/tips): {len(active_user_ids)}\")\n",
                "print(f\"Stage 5 Output Users (Cleaned): {user_output_count}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Stage 6: Summary Report\n",
                "\n",
                "Consolidate statistics collected from all stages and write the final report to `noise_cleaning_report.json`."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "2025-05-04 20:16:54,863 - INFO - --- Stage 6: Summary Report START ---\n",
                        "2025-05-04 20:16:54,866 - INFO - Successfully wrote summary report to ../noise_cleaning_report.json\n",
                        "2025-05-04 20:16:54,867 - INFO - --- Stage 6: Summary Report END --- Took 0.00 seconds ---\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "--- Pipeline Complete --- Summary ---\n",
                        "Total Execution Time: 649.13 seconds (10.82 minutes)\n",
                        "\n",
                        "Input Counts (Original Files):\n",
                        "  Businesses: 150346\n",
                        "  Reviews:    6990280\n",
                        "  Tips:       908915\n",
                        "  Users:      1987897\n",
                        "\n",
                        "Input Counts (After Business Filtering - Base for Retention):\n",
                        "  Reviews (Valid Biz): 5259993\n",
                        "  Tips (Valid Biz):    741599\n",
                        "\n",
                        "Final Output Counts (Cleaned):\n",
                        "  Businesses: 68696 (45.69%)\n",
                        "  Reviews:    5084370 (96.66%)\n",
                        "  Tips:       741599 (100.00%)\n",
                        "  Users:      1677963 (84.41%)\n",
                        "\n",
                        "Quality Alerts Triggered:\n",
                        "  - star_rating_shift_exceeded\n",
                        "\n",
                        "Detailed report saved to: ../noise_cleaning_report.json\n"
                    ]
                }
            ],
            "source": [
                "logging.info(\"--- Stage 6: Summary Report START ---\")\n",
                "start_time_stage6 = time.time()\n",
                "\n",
                "# --- Add Overall Summary Stats ---\n",
                "# Store overall timing using the initial start_time captured in the config cell\n",
                "pipeline_start_timestamp = start_time # Should be defined very early in the script\n",
                "report_stats['summary']['pipeline_start_time'] = datetime.datetime.fromtimestamp(pipeline_start_timestamp).strftime('%Y-%m-%d %H:%M:%S')\n",
                "report_stats['summary']['pipeline_end_time'] = datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
                "total_duration = (time.time() - pipeline_start_timestamp)\n",
                "report_stats['summary']['total_duration_seconds'] = round(total_duration, 2)\n",
                "\n",
                "# Calculate overall percentages using correct initial counts\n",
                "initial_biz = report_stats['stage_1_business']['input_count']\n",
                "final_biz = report_stats['stage_1_business']['output_count']\n",
                "report_stats['summary']['business_retention_rate'] = round(final_biz / initial_biz, 4) if initial_biz > 0 else 0\n",
                "\n",
                "# Initial reviews are those from valid businesses input to Stage 3\n",
                "initial_rev = report_stats['stage_3_dedup']['input_valid_biz_reviews']\n",
                "final_rev = report_stats['stage_4_review']['output_count'] # Final count after Stage 4 heuristics\n",
                "report_stats['summary']['overall_review_retention_rate'] = round(final_rev / initial_rev, 4) if initial_rev > 0 else 0\n",
                "\n",
                "# Initial tips are those from valid businesses input to Stage 4c heuristic filter\n",
                "initial_tip = report_stats['stage_4_tip']['input_valid_biz_tips']\n",
                "final_tip = report_stats['stage_4_tip']['output_count'] # Final count after Stage 4 heuristics\n",
                "report_stats['summary']['overall_tip_retention_rate'] = round(final_tip / initial_tip, 4) if initial_tip > 0 else 0\n",
                "\n",
                "initial_usr = report_stats['stage_5_user']['input_count']\n",
                "final_usr = report_stats['stage_5_user']['output_count']\n",
                "report_stats['summary']['user_retention_rate'] = round(final_usr / initial_usr, 4) if initial_usr > 0 else 0\n",
                "\n",
                "# Add final counts to summary for easy access\n",
                "report_stats['summary']['final_business_count'] = final_biz\n",
                "report_stats['summary']['final_review_count'] = final_rev\n",
                "report_stats['summary']['final_tip_count'] = final_tip\n",
                "report_stats['summary']['final_user_count'] = final_usr\n",
                "\n",
                "# Convert defaultdict to regular dict for JSON serialization\n",
                "# Use a helper function to handle nested defaultdicts properly\n",
                "def defaultdict_to_dict(d):\n",
                "    if isinstance(d, defaultdict):\n",
                "        d = {k: defaultdict_to_dict(v) for k, v in d.items()}\n",
                "    return d\n",
                "\n",
                "final_report = defaultdict_to_dict(report_stats)\n",
                "\n",
                "# --- Write Report File ---\n",
                "try:\n",
                "    with open(REPORT_FILE, 'w', encoding='utf-8') as f_report:\n",
                "        json.dump(final_report, f_report, indent=4)\n",
                "    logging.info(f\"Successfully wrote summary report to {REPORT_FILE}\")\n",
                "except Exception as e:\n",
                "    logging.error(f\"Failed to write summary report: {e}\", exc_info=True)\n",
                "\n",
                "duration_stage6 = time.time() - start_time_stage6\n",
                "logging.info(f\"--- Stage 6: Summary Report END --- Took {duration_stage6:.2f} seconds ---\")\n",
                "\n",
                "# --- Display Final Summary in Notebook ---\n",
                "print(\"\\n--- Pipeline Complete --- Summary ---\")\n",
                "print(f\"Total Execution Time: {total_duration:.2f} seconds ({total_duration/60:.2f} minutes)\")\n",
                "print(\"\\nInput Counts (Original Files):\")\n",
                "print(f\"  Businesses: {initial_biz}\")\n",
                "print(f\"  Reviews:    {report_stats['stage_3_dedup']['input_reviews_read']}\") # Total reviews read in Stage 3\n",
                "print(f\"  Tips:       {report_stats['stage_4_tip']['input_tips_read_total']}\") # Total tips read in Stage 4c\n",
                "print(f\"  Users:      {initial_usr}\")\n",
                "\n",
                "print(\"\\nInput Counts (After Business Filtering - Base for Retention):\")\n",
                "print(f\"  Reviews (Valid Biz): {initial_rev}\")\n",
                "print(f\"  Tips (Valid Biz):    {initial_tip}\")\n",
                "\n",
                "print(\"\\nFinal Output Counts (Cleaned):\")\n",
                "print(f\"  Businesses: {final_biz} ({report_stats['summary']['business_retention_rate']:.2%})\")\n",
                "print(f\"  Reviews:    {final_rev} ({report_stats['summary']['overall_review_retention_rate']:.2%})\")\n",
                "print(f\"  Tips:       {final_tip} ({report_stats['summary']['overall_tip_retention_rate']:.2%})\")\n",
                "print(f\"  Users:      {final_usr} ({report_stats['summary']['user_retention_rate']:.2%})\")\n",
                "\n",
                "\n",
                "# Check for the existence of the quality_alerts key before iterating\n",
                "alerts_triggered = final_report.get('quality_alerts', {})\n",
                "if any(alerts_triggered.values()):\n",
                "    print(\"\\nQuality Alerts Triggered:\")\n",
                "    for alert, triggered in alerts_triggered.items():\n",
                "        if triggered:\n",
                "            print(f\"  - {alert}\")\n",
                "else:\n",
                "    print(\"\\nNo quality alerts triggered.\")\n",
                "\n",
                "print(f\"\\nDetailed report saved to: {REPORT_FILE}\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.9.7"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}
